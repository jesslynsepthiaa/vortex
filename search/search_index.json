{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Vortex Documentation \u00b6 A Deep Learning Model Development Framework for Computer Vision Version : 0.1.0 Overview \u00b6 Vortex (a.k.a Visual Cortex) is a computer deep learning framework based on Pytorch that provides end-to-end model development. It utilizes a single experiment file in YAML format (and an optional YAML file for hyperparameter optimization) to navigate all of the pipelines and provides complete development environment which consists of the following pipelines : Training Validation Prediction Hyperparameters Optimization Additionally, it also support exporting Pytorch model into graph Intermediate Representation (IR) and utilize it on specific runtime by the following process : Graph Export IR Validation IR Prediction Currently we support deep learning model development on computer vision of the following task: Image Classification Object Detection ( UNVERIFIED YET, MAY PRODUCE BAD RESULTS ) Multiple Object Detection Single Object Detection with Landmarks Highlights \u00b6 Easy CLI usage Modular design, reusable components Various architecture support 50 + infamous backbone networks Classification and Detection architecture support Integration with image augmentation library ( Albumentations ) Integration with hyperparameter optimization library ( Optuna ) Integration with 3rd party experiment logger ( Comet.ml ) Graph export to Torchscript and ONNX Visual report of model's performance and resource usage, see this example Installation \u00b6 Currently this installation guide cover the installation of the following components: Vortex Vortex Runtime : torchscript (backends : cpu,cuda ) onnxruntime (backends : cpu ) On Host \u00b6 Install package dependencies : apt update apt install -y libsm6 libxext6 libxrender-dev ffmpeg \\ x264 libx264-dev libsm6 git sqlite3 \\ libsqlite3-dev graphviz pciutils Vortex run and tested on Python 3.6, so if you use Ubuntu 18.04 just run : apt install -y python3 python3-pip or, you can download the Python 3.6 release here And it's also better to update the pip package to avoid several installation issues pip3 install -U pip Then, clone the repo and install the vortex package git clone https://github.com/nodefluxio/vortex.git cd vortex git checkout v0.1.0 pip3 install '.' Additionally if you want to install vortex with optuna visualization support : pip3 install '.[optuna_vis]' This command will register a python package named as visual-cortex when you check it using pip3 list . pip3 list | grep visual-cortex /* visual-cortex {installed-version} */ To check whether the installation is succesful, you can run : python3.6 -c 'import vortex' Which will print output like this UserWarning: finished scanning dataset, available dataset(s) : { 'external': [], 'torchvision.datasets': [ 'MNIST', 'FashionMNIST', 'KMNIST', 'EMNIST', 'QMNIST', 'ImageFolder', 'CIFAR10', 'CIFAR100', 'SVHN', 'STL10']} (pp.pformat(all_datasets))) Using Docker \u00b6 You can build the dockerfile, docker build -t vortex:dev -f dockerfile . Getting Started \u00b6 Vortex utilizes a certain standard to allow seamless integration between pipelines. In this guide, we will show you how to integrate your dataset/use the built-in ones , how to build the experiment file , and how to utilize and propagate both items to all of Vortex pipelines. Developing Vortex Model \u00b6 The first step is dataset integration, it is recommended for you to check the built-in datasets section in order to find suitable setting for your dataset. For example, you can use torchvision's ImageFolder to integrate a classification dataset. However, if you didn't find any suitable internal integration, you can follow dataset integration section to make your own integration point Next, we need to build the experiment file , please follow the experiment file configuration section At this point, you should've already prepared your experiment file and your dataset . You can now run the training pipeline. See training pipeline section for further instructions. After receiving Vortex model from training pipeline, you can either do : measure your model's performance using validation pipeline , or directly use the model in your script using prediction pipeline API , or further optimize your model by converting it into Intermediate Representation using graph export pipeline If you choose to export your model, once you have the Vortex IR model, you can either do : measure your IR model's performance using IR validation pipeline , or directly use the IR model in your script using IR prediction pipeline API Hyperparameter Optimization \u00b6 Now, once you've accustomed with Vortex pipelines, you can explore the use of hypopt pipeline to find the best hyperparameter setting for your model. Basically To do that, you can follow the guide below : Prepare hyperparameter configuration file. You can check hypopt config file section to create it Make sure all requirement related to objective is already met. For example, if you want to use ValidationObjective , you need to check whether you've already prepared the requirements of validation pipeline Run the hypopt pipeline, see hypopt pipeline section Once you get the optimal hyperparameters value, you can use it with the corresponding pipeline","title":"Home"},{"location":"#vortex-documentation","text":"A Deep Learning Model Development Framework for Computer Vision Version : 0.1.0","title":"Vortex Documentation"},{"location":"#overview","text":"Vortex (a.k.a Visual Cortex) is a computer deep learning framework based on Pytorch that provides end-to-end model development. It utilizes a single experiment file in YAML format (and an optional YAML file for hyperparameter optimization) to navigate all of the pipelines and provides complete development environment which consists of the following pipelines : Training Validation Prediction Hyperparameters Optimization Additionally, it also support exporting Pytorch model into graph Intermediate Representation (IR) and utilize it on specific runtime by the following process : Graph Export IR Validation IR Prediction Currently we support deep learning model development on computer vision of the following task: Image Classification Object Detection ( UNVERIFIED YET, MAY PRODUCE BAD RESULTS ) Multiple Object Detection Single Object Detection with Landmarks","title":"Overview"},{"location":"#highlights","text":"Easy CLI usage Modular design, reusable components Various architecture support 50 + infamous backbone networks Classification and Detection architecture support Integration with image augmentation library ( Albumentations ) Integration with hyperparameter optimization library ( Optuna ) Integration with 3rd party experiment logger ( Comet.ml ) Graph export to Torchscript and ONNX Visual report of model's performance and resource usage, see this example","title":"Highlights"},{"location":"#installation","text":"Currently this installation guide cover the installation of the following components: Vortex Vortex Runtime : torchscript (backends : cpu,cuda ) onnxruntime (backends : cpu )","title":"Installation"},{"location":"#on-host","text":"Install package dependencies : apt update apt install -y libsm6 libxext6 libxrender-dev ffmpeg \\ x264 libx264-dev libsm6 git sqlite3 \\ libsqlite3-dev graphviz pciutils Vortex run and tested on Python 3.6, so if you use Ubuntu 18.04 just run : apt install -y python3 python3-pip or, you can download the Python 3.6 release here And it's also better to update the pip package to avoid several installation issues pip3 install -U pip Then, clone the repo and install the vortex package git clone https://github.com/nodefluxio/vortex.git cd vortex git checkout v0.1.0 pip3 install '.' Additionally if you want to install vortex with optuna visualization support : pip3 install '.[optuna_vis]' This command will register a python package named as visual-cortex when you check it using pip3 list . pip3 list | grep visual-cortex /* visual-cortex {installed-version} */ To check whether the installation is succesful, you can run : python3.6 -c 'import vortex' Which will print output like this UserWarning: finished scanning dataset, available dataset(s) : { 'external': [], 'torchvision.datasets': [ 'MNIST', 'FashionMNIST', 'KMNIST', 'EMNIST', 'QMNIST', 'ImageFolder', 'CIFAR10', 'CIFAR100', 'SVHN', 'STL10']} (pp.pformat(all_datasets)))","title":"On Host"},{"location":"#using-docker","text":"You can build the dockerfile, docker build -t vortex:dev -f dockerfile .","title":"Using Docker"},{"location":"#getting-started","text":"Vortex utilizes a certain standard to allow seamless integration between pipelines. In this guide, we will show you how to integrate your dataset/use the built-in ones , how to build the experiment file , and how to utilize and propagate both items to all of Vortex pipelines.","title":"Getting Started"},{"location":"#developing-vortex-model","text":"The first step is dataset integration, it is recommended for you to check the built-in datasets section in order to find suitable setting for your dataset. For example, you can use torchvision's ImageFolder to integrate a classification dataset. However, if you didn't find any suitable internal integration, you can follow dataset integration section to make your own integration point Next, we need to build the experiment file , please follow the experiment file configuration section At this point, you should've already prepared your experiment file and your dataset . You can now run the training pipeline. See training pipeline section for further instructions. After receiving Vortex model from training pipeline, you can either do : measure your model's performance using validation pipeline , or directly use the model in your script using prediction pipeline API , or further optimize your model by converting it into Intermediate Representation using graph export pipeline If you choose to export your model, once you have the Vortex IR model, you can either do : measure your IR model's performance using IR validation pipeline , or directly use the IR model in your script using IR prediction pipeline API","title":"Developing Vortex Model"},{"location":"#hyperparameter-optimization","text":"Now, once you've accustomed with Vortex pipelines, you can explore the use of hypopt pipeline to find the best hyperparameter setting for your model. Basically To do that, you can follow the guide below : Prepare hyperparameter configuration file. You can check hypopt config file section to create it Make sure all requirement related to objective is already met. For example, if you want to use ValidationObjective , you need to check whether you've already prepared the requirements of validation pipeline Run the hypopt pipeline, see hypopt pipeline section Once you get the optimal hyperparameters value, you can use it with the corresponding pipeline","title":"Hyperparameter Optimization"},{"location":"api/vortex.core.pipelines/","text":"vortex.core.pipelines \u00b6 Classes \u00b6 GraphExportPipeline \u00b6 Vortex Graph Export Pipeline API __init__ \u00b6 def __init__( self, config : easydict.EasyDict, weights : typing.Union[str, pathlib.Path, NoneType] = None, ) Arguments : config EasyDict - dictionary parsed from Vortex experiment file weights Union[str,Path,None], optional - path to selected Vortex model's weight. If set to None, it will assume that final model weights exist in experiment directory . Defaults to None. Examples : from vortex.utils.parser import load_config from vortex.core.pipelines import GraphExportPipeline # Parse config config = load_config('experiments/config/example.yml') graph_exporter=GraphExportPipeline(config=config,weights='experiments/outputs/example/example.pth') run \u00b6 def run( self, example_input : typing.Union[str, pathlib.Path, NoneType] = None, ) Arguments : example_input Union[str,Path,None], optional - path to example input image to help graph tracing. Defaults to None. Returns : EasyDict - dictionary containing status of the export process Examples : example_input = 'image1.jpg' graph_exporter = GraphExportPipeline(config=config, weights='experiments/outputs/example/example.pth') result = graph_exporter.run(example_input=example_input) HypOptPipeline \u00b6 Vortex Hyperparameters Optimization Pipeline API __init__ \u00b6 def __init__( self, config : easydict.EasyDict, optconfig : easydict.EasyDict, weights : typing.Union[str, pathlib.Path, NoneType] = None, ) Arguments : config EasyDict - dictionary parsed from Vortex experiment file optconfig EasyDict - dictionary parsed from Vortex hypopt configuration file weights Union[str,Path,None], optional - path to selected Vortex model's weight. If set to None, it will assume that final model weights exist in experiment directory . Only used for ValidationObjective. Defaults to None. Examples : from vortex.core.pipelines import HypOptPipeline from vortex.utils.parser.loader import Loader import yaml # Parse config config_path = 'experiments/config/example.yml' optconfig_path = 'experiments/hypopt/learning_rate_search.yml' with open(config_path) as f: config_data = yaml.load(f, Loader=Loader) with open(optconfig_path) as f: optconfig_data = yaml.load(f, Loader=Loader) graph_exporter=HypOptPipeline(config=config,optconfig=optconfig) run \u00b6 def run( self, ) Returns : EasyDict - dictionary containing result of the hypopt process Examples : graph_exporter=HypOptPipeline(config=config,optconfig=optconfig) results=graph_exporter.run() PytorchPredictionPipeline \u00b6 Vortex Prediction Pipeline API for Vortex model __init__ \u00b6 def __init__( self, config : easydict.EasyDict, weights : typing.Union[str, pathlib.Path, NoneType] = None, device : typing.Union[str, NoneType] = None, ) Arguments : config EasyDict - dictionary parsed from Vortex experiment file weights Union[str,Path,None], optional - path to selected Vortex model's weight. If set to None, it will assume that final model weights exist in experiment directory . Defaults to None. device Union[str,None], optional - selected device for model's computation. If None, it will use the device described in experiment file . Defaults to None. Raises : FileNotFoundError - raise error if selected 'weights' file is not found Examples : from vortex.core.pipelines import PytorchPredictionPipeline from vortex.utils.parser import load_config # Parse config config_path = 'experiments/config/example.yml' config = load_config(config_path) weights_file = 'experiments/outputs/example/example.pth' device = 'cuda' vortex_predictor=PytorchPredictionPipeline(config = config, weights = weights_file, device = device) run \u00b6 def run( self, images : typing.Union[typing.List[str], numpy.ndarray], visualize : bool = False, dump_visual : bool = False, output_dir : typing.Union[str, pathlib.Path] = '.', **kwargs, ) Arguments : images Union[List[str],np.ndarray] - list of images path or array of image visualize bool, optional - option to return prediction visualization. Defaults to False. dump_visual bool, optional - option to dump prediction visualization. Defaults to False. output_dir Union[str,Path], optional - directory path to dump visualization. Defaults to '.' . kwargs optional - this kwargs is placement for additional input parameters specific to models'task Returns : EasyDict - dictionary of prediction result Raises : TypeError - raise error if provided 'images' is not list of image path or array of images Examples : # Initialize prediction pipeline vortex_predictor=PytorchPredictionPipeline(config = config, weights = weights_file, device = device) ### TODO: adding 'input_specs' ## OR vortex_predictor=IRPredictionPipeline(model = model_file, runtime = runtime) ### You can get model's required parameter by extracting ### model's 'input_specs' attributes input_shape = vortex_predictor.model.input_specs.shape additional_run_params = [key for key in vortex_predictor.model.input_specs.keys() if key!='input'] print(additional_run_params) #### Assume that the model is detection model #### ['score_threshold', 'iou_threshold'] << this parameter must be provided in run() arguments # Prepare batched input batch_input = ['image1.jpg','image2.jpg'] ## OR import cv2 batch_input = np.array([cv2.imread('image1.jpg'),cv2.imread('image2.jpg')]) results = vortex_predictor.run(images=batch_input, score_threshold=0.9, iou_threshold=0.2) IRPredictionPipeline \u00b6 Vortex Prediction Pipeline API for Vortex IR model __init__ \u00b6 def __init__( self, model : typing.Union[str, pathlib.Path], runtime : str = 'cpu', ) Arguments : model Union[str,Path] - path to Vortex IR model, file with extension '.onnx' or '.pt' runtime str, optional - backend runtime to be selected for model's computation. Defaults to 'cpu'. Examples : from vortex.core.pipelines import IRPredictionPipeline from vortex.utils.parser import load_config # Parse config model_file = 'experiments/outputs/example/example.pt' # Model file with extension '.onnx' or '.pt' runtime = 'cpu' vortex_predictor=IRPredictionPipeline(model = model_file, runtime = runtime) run \u00b6 def run( self, images : typing.Union[typing.List[str], numpy.ndarray], visualize : bool = False, dump_visual : bool = False, output_dir : typing.Union[str, pathlib.Path] = '.', **kwargs, ) Arguments : images Union[List[str],np.ndarray] - list of images path or array of image visualize bool, optional - option to return prediction visualization. Defaults to False. dump_visual bool, optional - option to dump prediction visualization. Defaults to False. output_dir Union[str,Path], optional - directory path to dump visualization. Defaults to '.' . kwargs optional - this kwargs is placement for additional input parameters specific to models'task Returns : EasyDict - dictionary of prediction result Raises : TypeError - raise error if provided 'images' is not list of image path or array of images Examples : # Initialize prediction pipeline vortex_predictor=PytorchPredictionPipeline(config = config, weights = weights_file, device = device) ### TODO: adding 'input_specs' ## OR vortex_predictor=IRPredictionPipeline(model = model_file, runtime = runtime) ### You can get model's required parameter by extracting ### model's 'input_specs' attributes input_shape = vortex_predictor.model.input_specs.shape additional_run_params = [key for key in vortex_predictor.model.input_specs.keys() if key!='input'] print(additional_run_params) #### Assume that the model is detection model #### ['score_threshold', 'iou_threshold'] << this parameter must be provided in run() arguments # Prepare batched input batch_input = ['image1.jpg','image2.jpg'] ## OR import cv2 batch_input = np.array([cv2.imread('image1.jpg'),cv2.imread('image2.jpg')]) results = vortex_predictor.run(images=batch_input, score_threshold=0.9, iou_threshold=0.2) runtime_predict \u00b6 def runtime_predict( predictor, image : numpy.ndarray, **kwargs, ) Arguments : predictor - Vortex runtime object image np.ndarray - array of batched input image(s) with dimension of 4 (n,h,w,c) kwargs optional - this kwargs is placement for additional input parameters specific to models'task Returns : List - list of prediction results Examples : from vortex.core.factory import create_runtime_model from vortex.core.pipelines import IRPredictionPipeline import cv2 model_file = 'experiments/outputs/example/example_bs2.pt' # Model file with extension '.onnx' or '.pt' runtime = 'cpu' model = create_runtime_model(model_file, runtime) batch_imgs = np.array([cv2.imread('image1.jpg'),cv2.imread('image2.jpg')]) results = IRPredictionPipeline.runtime_predict(model, batch_imgs, score_threshold=0.9, iou_threshold=0.2 ) TrainingPipeline \u00b6 Vortex Training Pipeline API __init__ \u00b6 def __init__( self, config : easydict.EasyDict, config_path : typing.Union[str, pathlib.Path, NoneType] = None, hypopt : bool = False, ) Arguments : config EasyDict - dictionary parsed from Vortex experiment file config_path Union[str,Path,None], optional - path to experiment file. Need to be provided for backup experiment file . Defaults to None. hypopt bool, optional - flag for hypopt, disable several pipeline process. Defaults to False. Raises : Exception - raise undocumented error if exist Examples : from vortex.utils.parser import load_config from vortex.core.pipelines import TrainingPipeline # Parse config config_path = 'experiments/config/example.yml' config = load_config(config_path) train_executor = TrainingPipeline(config=config,config_path=config_path,hypopt=False) run \u00b6 def run( self, save_model : bool = True, ) Arguments : save_model bool, optional - dump model's checkpoint. Defaults to True. Returns : EasyDict - dictionary containing loss, val results and learning rates history Examples : train_executor = TrainingPipeline(config=config,config_path=config_path,hypopt=False) outputs = train_executor.run() PytorchValidationPipeline \u00b6 Vortex Validation Pipeline API for Vortex model __init__ \u00b6 def __init__( self, config : easydict.EasyDict, weights : typing.Union[str, pathlib.Path, NoneType] = None, backends : typing.Union[list, str] = [], generate_report : bool = True, hypopt : bool = False, ) Arguments : config EasyDict - dictionary parsed from Vortex experiment file weights Union[str,Path,None], optional - path to selected Vortex model's weight. If set to None, it will assume that final model weights exist in experiment directory . Defaults to None. backends Union[list,str], optional - device(s) to be used for validation process. If not provided, it will use the device described in experiment file . Defaults to []. generate_report bool, optional - if enabled will generate validation report in markdown format. Defaults to True. hypopt bool, optional - flag for hypopt, disable several pipeline process. Defaults to False. Examples : from vortex.utils.parser import load_config from vortex.core.pipelines import PytorchValidationPipeline # Parse config config_path = 'experiments/config/example.yml' weights_file = 'experiments/outputs/example/example.pth' backends = ['cpu','cuda'] config = load_config(config_path) validation_executor = PytorchValidationPipeline(config=config, weights = weights_file, backends = backends, generate_report = True) run \u00b6 def run( self, batch_size : int = 1, ) Arguments : batch_size int, optional - size of validation input batch. Defaults to 1. Returns : EasyDict - dictionary containing validation metrics result Examples : # Initialize validation pipeline validation_executor = PytorchValidationPipeline(config=config, weights = weights_file, backends = backends, generate_report = True) ## OR validation_executor = IRValidationPipeline(config=config, model = model_file, backends = backends, generate_report = True) # Run validation process results = validation_executor.run(batch_size = 2) ## OR (Currently for IRValidationPipeline only) ## 'batch_size' information is also embedded in model.input_specs['input']['shape'][0] batch_size = validation_executor.model.input_specs['input']['shape'][0] results = validation_executor.run(batch_size = batch_size) IRValidationPipeline \u00b6 Vortex Validation Pipeline API for Vortex IR model __init__ \u00b6 def __init__( self, config : easydict.EasyDict, model : typing.Union[str, pathlib.Path, NoneType], backends : typing.Union[list, str] = ['cpu'], generate_report : bool = True, hypopt : bool = False, ) Arguments : config EasyDict - ictionary parsed from Vortex experiment file model Union[str,Path,None] - path to Vortex IR model, file with extension '.onnx' or '.pt' backends Union[list,str], optional - runtime(s) to be used for validation process. Defaults to ['cpu']. generate_report bool, optional - if enabled will generate validation report in markdown format. Defaults to True. hypopt bool, optional - flag for hypopt, disable several pipeline process. Defaults to False. Raises : RuntimeError - raise error if the provided model file's extension is not ' .onnx' or ' .pt' Examples : from vortex.utils.parser import load_config from vortex.core.pipelines import IRValidationPipeline # Parse config config_path = 'experiments/config/example.yml' model_file = 'experiments/outputs/example/example.pt' backends = ['cpu','cuda'] config = load_config(config_path) validation_executor = IRValidationPipeline(config=config, model = model_file, backends = backends, generate_report = True) run \u00b6 def run( self, batch_size : int = 1, ) Arguments : batch_size int, optional - size of validation input batch. Defaults to 1. Returns : EasyDict - dictionary containing validation metrics result Examples : # Initialize validation pipeline validation_executor = PytorchValidationPipeline(config=config, weights = weights_file, backends = backends, generate_report = True) ## OR validation_executor = IRValidationPipeline(config=config, model = model_file, backends = backends, generate_report = True) # Run validation process results = validation_executor.run(batch_size = 2) ## OR (Currently for IRValidationPipeline only) ## 'batch_size' information is also embedded in model.input_specs['input']['shape'][0] batch_size = validation_executor.model.input_specs['input']['shape'][0] results = validation_executor.run(batch_size = batch_size)","title":"vortex.core.pipelines"},{"location":"api/vortex.core.pipelines/#vortexcorepipelines","text":"","title":"vortex.core.pipelines"},{"location":"api/vortex.core.pipelines/#classes","text":"","title":"Classes"},{"location":"api/vortex.core.pipelines/#graphexportpipeline","text":"Vortex Graph Export Pipeline API","title":"GraphExportPipeline"},{"location":"api/vortex.core.pipelines/#__init__","text":"def __init__( self, config : easydict.EasyDict, weights : typing.Union[str, pathlib.Path, NoneType] = None, ) Arguments : config EasyDict - dictionary parsed from Vortex experiment file weights Union[str,Path,None], optional - path to selected Vortex model's weight. If set to None, it will assume that final model weights exist in experiment directory . Defaults to None. Examples : from vortex.utils.parser import load_config from vortex.core.pipelines import GraphExportPipeline # Parse config config = load_config('experiments/config/example.yml') graph_exporter=GraphExportPipeline(config=config,weights='experiments/outputs/example/example.pth')","title":"__init__"},{"location":"api/vortex.core.pipelines/#run","text":"def run( self, example_input : typing.Union[str, pathlib.Path, NoneType] = None, ) Arguments : example_input Union[str,Path,None], optional - path to example input image to help graph tracing. Defaults to None. Returns : EasyDict - dictionary containing status of the export process Examples : example_input = 'image1.jpg' graph_exporter = GraphExportPipeline(config=config, weights='experiments/outputs/example/example.pth') result = graph_exporter.run(example_input=example_input)","title":"run"},{"location":"api/vortex.core.pipelines/#hypoptpipeline","text":"Vortex Hyperparameters Optimization Pipeline API","title":"HypOptPipeline"},{"location":"api/vortex.core.pipelines/#__init___1","text":"def __init__( self, config : easydict.EasyDict, optconfig : easydict.EasyDict, weights : typing.Union[str, pathlib.Path, NoneType] = None, ) Arguments : config EasyDict - dictionary parsed from Vortex experiment file optconfig EasyDict - dictionary parsed from Vortex hypopt configuration file weights Union[str,Path,None], optional - path to selected Vortex model's weight. If set to None, it will assume that final model weights exist in experiment directory . Only used for ValidationObjective. Defaults to None. Examples : from vortex.core.pipelines import HypOptPipeline from vortex.utils.parser.loader import Loader import yaml # Parse config config_path = 'experiments/config/example.yml' optconfig_path = 'experiments/hypopt/learning_rate_search.yml' with open(config_path) as f: config_data = yaml.load(f, Loader=Loader) with open(optconfig_path) as f: optconfig_data = yaml.load(f, Loader=Loader) graph_exporter=HypOptPipeline(config=config,optconfig=optconfig)","title":"__init__"},{"location":"api/vortex.core.pipelines/#run_1","text":"def run( self, ) Returns : EasyDict - dictionary containing result of the hypopt process Examples : graph_exporter=HypOptPipeline(config=config,optconfig=optconfig) results=graph_exporter.run()","title":"run"},{"location":"api/vortex.core.pipelines/#pytorchpredictionpipeline","text":"Vortex Prediction Pipeline API for Vortex model","title":"PytorchPredictionPipeline"},{"location":"api/vortex.core.pipelines/#__init___2","text":"def __init__( self, config : easydict.EasyDict, weights : typing.Union[str, pathlib.Path, NoneType] = None, device : typing.Union[str, NoneType] = None, ) Arguments : config EasyDict - dictionary parsed from Vortex experiment file weights Union[str,Path,None], optional - path to selected Vortex model's weight. If set to None, it will assume that final model weights exist in experiment directory . Defaults to None. device Union[str,None], optional - selected device for model's computation. If None, it will use the device described in experiment file . Defaults to None. Raises : FileNotFoundError - raise error if selected 'weights' file is not found Examples : from vortex.core.pipelines import PytorchPredictionPipeline from vortex.utils.parser import load_config # Parse config config_path = 'experiments/config/example.yml' config = load_config(config_path) weights_file = 'experiments/outputs/example/example.pth' device = 'cuda' vortex_predictor=PytorchPredictionPipeline(config = config, weights = weights_file, device = device)","title":"__init__"},{"location":"api/vortex.core.pipelines/#run_2","text":"def run( self, images : typing.Union[typing.List[str], numpy.ndarray], visualize : bool = False, dump_visual : bool = False, output_dir : typing.Union[str, pathlib.Path] = '.', **kwargs, ) Arguments : images Union[List[str],np.ndarray] - list of images path or array of image visualize bool, optional - option to return prediction visualization. Defaults to False. dump_visual bool, optional - option to dump prediction visualization. Defaults to False. output_dir Union[str,Path], optional - directory path to dump visualization. Defaults to '.' . kwargs optional - this kwargs is placement for additional input parameters specific to models'task Returns : EasyDict - dictionary of prediction result Raises : TypeError - raise error if provided 'images' is not list of image path or array of images Examples : # Initialize prediction pipeline vortex_predictor=PytorchPredictionPipeline(config = config, weights = weights_file, device = device) ### TODO: adding 'input_specs' ## OR vortex_predictor=IRPredictionPipeline(model = model_file, runtime = runtime) ### You can get model's required parameter by extracting ### model's 'input_specs' attributes input_shape = vortex_predictor.model.input_specs.shape additional_run_params = [key for key in vortex_predictor.model.input_specs.keys() if key!='input'] print(additional_run_params) #### Assume that the model is detection model #### ['score_threshold', 'iou_threshold'] << this parameter must be provided in run() arguments # Prepare batched input batch_input = ['image1.jpg','image2.jpg'] ## OR import cv2 batch_input = np.array([cv2.imread('image1.jpg'),cv2.imread('image2.jpg')]) results = vortex_predictor.run(images=batch_input, score_threshold=0.9, iou_threshold=0.2)","title":"run"},{"location":"api/vortex.core.pipelines/#irpredictionpipeline","text":"Vortex Prediction Pipeline API for Vortex IR model","title":"IRPredictionPipeline"},{"location":"api/vortex.core.pipelines/#__init___3","text":"def __init__( self, model : typing.Union[str, pathlib.Path], runtime : str = 'cpu', ) Arguments : model Union[str,Path] - path to Vortex IR model, file with extension '.onnx' or '.pt' runtime str, optional - backend runtime to be selected for model's computation. Defaults to 'cpu'. Examples : from vortex.core.pipelines import IRPredictionPipeline from vortex.utils.parser import load_config # Parse config model_file = 'experiments/outputs/example/example.pt' # Model file with extension '.onnx' or '.pt' runtime = 'cpu' vortex_predictor=IRPredictionPipeline(model = model_file, runtime = runtime)","title":"__init__"},{"location":"api/vortex.core.pipelines/#run_3","text":"def run( self, images : typing.Union[typing.List[str], numpy.ndarray], visualize : bool = False, dump_visual : bool = False, output_dir : typing.Union[str, pathlib.Path] = '.', **kwargs, ) Arguments : images Union[List[str],np.ndarray] - list of images path or array of image visualize bool, optional - option to return prediction visualization. Defaults to False. dump_visual bool, optional - option to dump prediction visualization. Defaults to False. output_dir Union[str,Path], optional - directory path to dump visualization. Defaults to '.' . kwargs optional - this kwargs is placement for additional input parameters specific to models'task Returns : EasyDict - dictionary of prediction result Raises : TypeError - raise error if provided 'images' is not list of image path or array of images Examples : # Initialize prediction pipeline vortex_predictor=PytorchPredictionPipeline(config = config, weights = weights_file, device = device) ### TODO: adding 'input_specs' ## OR vortex_predictor=IRPredictionPipeline(model = model_file, runtime = runtime) ### You can get model's required parameter by extracting ### model's 'input_specs' attributes input_shape = vortex_predictor.model.input_specs.shape additional_run_params = [key for key in vortex_predictor.model.input_specs.keys() if key!='input'] print(additional_run_params) #### Assume that the model is detection model #### ['score_threshold', 'iou_threshold'] << this parameter must be provided in run() arguments # Prepare batched input batch_input = ['image1.jpg','image2.jpg'] ## OR import cv2 batch_input = np.array([cv2.imread('image1.jpg'),cv2.imread('image2.jpg')]) results = vortex_predictor.run(images=batch_input, score_threshold=0.9, iou_threshold=0.2)","title":"run"},{"location":"api/vortex.core.pipelines/#runtime_predict","text":"def runtime_predict( predictor, image : numpy.ndarray, **kwargs, ) Arguments : predictor - Vortex runtime object image np.ndarray - array of batched input image(s) with dimension of 4 (n,h,w,c) kwargs optional - this kwargs is placement for additional input parameters specific to models'task Returns : List - list of prediction results Examples : from vortex.core.factory import create_runtime_model from vortex.core.pipelines import IRPredictionPipeline import cv2 model_file = 'experiments/outputs/example/example_bs2.pt' # Model file with extension '.onnx' or '.pt' runtime = 'cpu' model = create_runtime_model(model_file, runtime) batch_imgs = np.array([cv2.imread('image1.jpg'),cv2.imread('image2.jpg')]) results = IRPredictionPipeline.runtime_predict(model, batch_imgs, score_threshold=0.9, iou_threshold=0.2 )","title":"runtime_predict"},{"location":"api/vortex.core.pipelines/#trainingpipeline","text":"Vortex Training Pipeline API","title":"TrainingPipeline"},{"location":"api/vortex.core.pipelines/#__init___4","text":"def __init__( self, config : easydict.EasyDict, config_path : typing.Union[str, pathlib.Path, NoneType] = None, hypopt : bool = False, ) Arguments : config EasyDict - dictionary parsed from Vortex experiment file config_path Union[str,Path,None], optional - path to experiment file. Need to be provided for backup experiment file . Defaults to None. hypopt bool, optional - flag for hypopt, disable several pipeline process. Defaults to False. Raises : Exception - raise undocumented error if exist Examples : from vortex.utils.parser import load_config from vortex.core.pipelines import TrainingPipeline # Parse config config_path = 'experiments/config/example.yml' config = load_config(config_path) train_executor = TrainingPipeline(config=config,config_path=config_path,hypopt=False)","title":"__init__"},{"location":"api/vortex.core.pipelines/#run_4","text":"def run( self, save_model : bool = True, ) Arguments : save_model bool, optional - dump model's checkpoint. Defaults to True. Returns : EasyDict - dictionary containing loss, val results and learning rates history Examples : train_executor = TrainingPipeline(config=config,config_path=config_path,hypopt=False) outputs = train_executor.run()","title":"run"},{"location":"api/vortex.core.pipelines/#pytorchvalidationpipeline","text":"Vortex Validation Pipeline API for Vortex model","title":"PytorchValidationPipeline"},{"location":"api/vortex.core.pipelines/#__init___5","text":"def __init__( self, config : easydict.EasyDict, weights : typing.Union[str, pathlib.Path, NoneType] = None, backends : typing.Union[list, str] = [], generate_report : bool = True, hypopt : bool = False, ) Arguments : config EasyDict - dictionary parsed from Vortex experiment file weights Union[str,Path,None], optional - path to selected Vortex model's weight. If set to None, it will assume that final model weights exist in experiment directory . Defaults to None. backends Union[list,str], optional - device(s) to be used for validation process. If not provided, it will use the device described in experiment file . Defaults to []. generate_report bool, optional - if enabled will generate validation report in markdown format. Defaults to True. hypopt bool, optional - flag for hypopt, disable several pipeline process. Defaults to False. Examples : from vortex.utils.parser import load_config from vortex.core.pipelines import PytorchValidationPipeline # Parse config config_path = 'experiments/config/example.yml' weights_file = 'experiments/outputs/example/example.pth' backends = ['cpu','cuda'] config = load_config(config_path) validation_executor = PytorchValidationPipeline(config=config, weights = weights_file, backends = backends, generate_report = True)","title":"__init__"},{"location":"api/vortex.core.pipelines/#run_5","text":"def run( self, batch_size : int = 1, ) Arguments : batch_size int, optional - size of validation input batch. Defaults to 1. Returns : EasyDict - dictionary containing validation metrics result Examples : # Initialize validation pipeline validation_executor = PytorchValidationPipeline(config=config, weights = weights_file, backends = backends, generate_report = True) ## OR validation_executor = IRValidationPipeline(config=config, model = model_file, backends = backends, generate_report = True) # Run validation process results = validation_executor.run(batch_size = 2) ## OR (Currently for IRValidationPipeline only) ## 'batch_size' information is also embedded in model.input_specs['input']['shape'][0] batch_size = validation_executor.model.input_specs['input']['shape'][0] results = validation_executor.run(batch_size = batch_size)","title":"run"},{"location":"api/vortex.core.pipelines/#irvalidationpipeline","text":"Vortex Validation Pipeline API for Vortex IR model","title":"IRValidationPipeline"},{"location":"api/vortex.core.pipelines/#__init___6","text":"def __init__( self, config : easydict.EasyDict, model : typing.Union[str, pathlib.Path, NoneType], backends : typing.Union[list, str] = ['cpu'], generate_report : bool = True, hypopt : bool = False, ) Arguments : config EasyDict - ictionary parsed from Vortex experiment file model Union[str,Path,None] - path to Vortex IR model, file with extension '.onnx' or '.pt' backends Union[list,str], optional - runtime(s) to be used for validation process. Defaults to ['cpu']. generate_report bool, optional - if enabled will generate validation report in markdown format. Defaults to True. hypopt bool, optional - flag for hypopt, disable several pipeline process. Defaults to False. Raises : RuntimeError - raise error if the provided model file's extension is not ' .onnx' or ' .pt' Examples : from vortex.utils.parser import load_config from vortex.core.pipelines import IRValidationPipeline # Parse config config_path = 'experiments/config/example.yml' model_file = 'experiments/outputs/example/example.pt' backends = ['cpu','cuda'] config = load_config(config_path) validation_executor = IRValidationPipeline(config=config, model = model_file, backends = backends, generate_report = True)","title":"__init__"},{"location":"api/vortex.core.pipelines/#run_6","text":"def run( self, batch_size : int = 1, ) Arguments : batch_size int, optional - size of validation input batch. Defaults to 1. Returns : EasyDict - dictionary containing validation metrics result Examples : # Initialize validation pipeline validation_executor = PytorchValidationPipeline(config=config, weights = weights_file, backends = backends, generate_report = True) ## OR validation_executor = IRValidationPipeline(config=config, model = model_file, backends = backends, generate_report = True) # Run validation process results = validation_executor.run(batch_size = 2) ## OR (Currently for IRValidationPipeline only) ## 'batch_size' information is also embedded in model.input_specs['input']['shape'][0] batch_size = validation_executor.model.input_specs['input']['shape'][0] results = validation_executor.run(batch_size = batch_size)","title":"run"},{"location":"modules/augmentation/","text":"Augmentations \u00b6 This section listed all available augmentations modules configurations. Part of dataset.train configurations in experiment file. Albumentations \u00b6 You can utilized integrated albumentations image augmentation library in Vortex. Originally users must code the following example to use it. E.g. : from albumentations.augmentations.transforms import ( HorizontalFlip, RandomScale, RandomBrightnessContrast, RandomSnow ) from albumentations.core.composition import ( Compose, OneOf, BboxParams ) bbox_params=BboxParams(format='coco',min_area=0.0, min_visibility=0.0) transforms=Compose([OneOf(transforms=[RandomBrightnessContrast(p=0.5), RandomSnow(p=0.5)],p=0.5), HorizontalFlip(p=0.5), RandomScale(p=0.5,scale_limit=0.1)], bbox_params=bbox_params) In Vortex we simplify it in the experiment file, and abstracted the process of each image and targets. So the analogous form of the above script in Vortex configuration is shown below. E.g. : augmentations: [ { module: albumentations, args: { transforms: [ { compose: OneOf, args: { transforms: [ { transform: RandomBrightnessContrast, args: { p: 0.5}}, { transform: RandomSnow, args: { p: 0.5}} ], p: 0.5} }, { transform: HorizontalFlip, args: { p: 0.5 } }, { transform: RandomScale, args: { scale_limit: 0.1, p: 0.5 } } ], bbox_params: { min_visibility: 0.0, min_area: 0.0 }, visual_debug: False } } ] Arguments : transforms (list[dict]) : list of augmentation transformation or compose to be sequentially added. Each member of the list is a dictionary with a sub-arguments shown below : compose OR transform (str) : denotes a compose or a transform from albumentation. Only support OneOf compose for compose key. Supported transform available in this link . args : the corresponding arguments for selected compose or transform for OneOf compose, the transforms arguments have same description with the previous transforms (list[dict]). Possible for nested compose inside transforms bbox_params (dict) : Parameter for bounding box target's annotation. See this link for further reading. Supported sub-args: min_visibility (float) : minimum fraction of area for a bounding box to remain this box in list min_area (float) : minimum area of a bounding box. All bounding boxes whose visible area in pixels is less than this value will be removed visual_debug (bool) : used for visualization debugging. It uses \u2018cv2.imshow\u2019 to visualize every augmentations result. Disable it for training, default False","title":"Augmentations"},{"location":"modules/augmentation/#augmentations","text":"This section listed all available augmentations modules configurations. Part of dataset.train configurations in experiment file.","title":"Augmentations"},{"location":"modules/augmentation/#albumentations","text":"You can utilized integrated albumentations image augmentation library in Vortex. Originally users must code the following example to use it. E.g. : from albumentations.augmentations.transforms import ( HorizontalFlip, RandomScale, RandomBrightnessContrast, RandomSnow ) from albumentations.core.composition import ( Compose, OneOf, BboxParams ) bbox_params=BboxParams(format='coco',min_area=0.0, min_visibility=0.0) transforms=Compose([OneOf(transforms=[RandomBrightnessContrast(p=0.5), RandomSnow(p=0.5)],p=0.5), HorizontalFlip(p=0.5), RandomScale(p=0.5,scale_limit=0.1)], bbox_params=bbox_params) In Vortex we simplify it in the experiment file, and abstracted the process of each image and targets. So the analogous form of the above script in Vortex configuration is shown below. E.g. : augmentations: [ { module: albumentations, args: { transforms: [ { compose: OneOf, args: { transforms: [ { transform: RandomBrightnessContrast, args: { p: 0.5}}, { transform: RandomSnow, args: { p: 0.5}} ], p: 0.5} }, { transform: HorizontalFlip, args: { p: 0.5 } }, { transform: RandomScale, args: { scale_limit: 0.1, p: 0.5 } } ], bbox_params: { min_visibility: 0.0, min_area: 0.0 }, visual_debug: False } } ] Arguments : transforms (list[dict]) : list of augmentation transformation or compose to be sequentially added. Each member of the list is a dictionary with a sub-arguments shown below : compose OR transform (str) : denotes a compose or a transform from albumentation. Only support OneOf compose for compose key. Supported transform available in this link . args : the corresponding arguments for selected compose or transform for OneOf compose, the transforms arguments have same description with the previous transforms (list[dict]). Possible for nested compose inside transforms bbox_params (dict) : Parameter for bounding box target's annotation. See this link for further reading. Supported sub-args: min_visibility (float) : minimum fraction of area for a bounding box to remain this box in list min_area (float) : minimum area of a bounding box. All bounding boxes whose visible area in pixels is less than this value will be removed visual_debug (bool) : used for visualization debugging. It uses \u2018cv2.imshow\u2019 to visualize every augmentations result. Disable it for training, default False","title":"Albumentations"},{"location":"modules/backbones/","text":"Backbones Network \u00b6 This section listed all available backbone configuration. Part of model.name configurations (if the model support it) in experiment file. Backbones refers to the network which takes as input the image and extracts the feature map upon which the rest of the network is based. Usually, a famous image classification network is used as a backbone for other computer vision tasks, such as object detection. For example we can see figure below : In the figure, Resnet is used as a backbone in the Feature Pyramid Network (FPN) model for object detection. In Vortex, we provide various backbone that can be combined in different models. Backbones params and MAC/FLOPS comparison ( to compare which backbones is the lightest or heaviest ) can be found on this spreadsheet link NOTES : Params : total parameters in the model\u2019s weight MAC / FLOPS : Multiply-Accumulate Operation / Floating Point Operations per Second Darknet \u00b6 Variants from reference YOLOv3: An Incremental Improvement : darknet53 EfficientNet \u00b6 The pretrained model for EfficientNet base model (b0 - l2) is trained using Noisy Student, except for b8 . Variants from reference EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks : efficientnet_b0 efficientnet_b1 efficientnet_b2 efficientnet_b3 efficientnet_b4 efficientnet_b5 efficientnet_b6 efficientnet_b7 efficientnet_b8 Variants from reference Self-training with Noisy Student improves ImageNet classification : efficientnet_l2 efficientnet_l2_475 Variants from reference EfficientNet-EdgeTPU: Creating Accelerator-Optimized Neural Networks with AutoML : efficientnet_edge_s efficientnet_edge_m efficientnet_edge_l Variants from reference Higher accuracy on vision models with EfficientNet-Lite : efficientnet_lite0 efficientnet_lite1 efficientnet_lite2 efficientnet_lite3 efficientnet_lite4 MobileNet \u00b6 Variants from reference MobileNetV2: Inverted Residuals and Linear Bottlenecks : mobilenet_v2 Variants from reference Searching for MobileNetV3 : mobilenetv3_small_w7d20 mobilenetv3_small_wd2 mobilenetv3_small_w3d4 mobilenetv3_small_w1 mobilenetv3_small_w5d4 mobilenetv3_large_w7d20 mobilenetv3_large_wd2 mobilenetv3_large_w3d4 mobilenetv3_large_w1 mobilenetv3_large_w5d4 ResNet \u00b6 Variants from reference Deep Residual Learning for Image Recognition : resnet18 resnet34 resnet50 resnet101 resnet152 Variants from reference Aggregated Residual Transformations for Deep Neural Networks : resnext50_32x4d resnext101_32x8d Variants from reference Wide Residual Networks : wide_resnet50_2 wide_resnet101_2 ShuffleNet \u00b6 Variants from reference ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design : shufflenetv2_x0.5 shufflenetv2_x1.0 shufflenetv2_x1.5 shufflenetv2_x2.0 VGG \u00b6 Variants from reference Very Deep Convolutional Networks for Large-Scale Image Recognition : vgg11 vgg11_bn vgg13 vgg13_bn vgg16 vgg16_bn vgg19 vgg19_bn","title":"Backbones Network"},{"location":"modules/backbones/#backbones-network","text":"This section listed all available backbone configuration. Part of model.name configurations (if the model support it) in experiment file. Backbones refers to the network which takes as input the image and extracts the feature map upon which the rest of the network is based. Usually, a famous image classification network is used as a backbone for other computer vision tasks, such as object detection. For example we can see figure below : In the figure, Resnet is used as a backbone in the Feature Pyramid Network (FPN) model for object detection. In Vortex, we provide various backbone that can be combined in different models. Backbones params and MAC/FLOPS comparison ( to compare which backbones is the lightest or heaviest ) can be found on this spreadsheet link NOTES : Params : total parameters in the model\u2019s weight MAC / FLOPS : Multiply-Accumulate Operation / Floating Point Operations per Second","title":"Backbones Network"},{"location":"modules/backbones/#darknet","text":"Variants from reference YOLOv3: An Incremental Improvement : darknet53","title":"Darknet"},{"location":"modules/backbones/#efficientnet","text":"The pretrained model for EfficientNet base model (b0 - l2) is trained using Noisy Student, except for b8 . Variants from reference EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks : efficientnet_b0 efficientnet_b1 efficientnet_b2 efficientnet_b3 efficientnet_b4 efficientnet_b5 efficientnet_b6 efficientnet_b7 efficientnet_b8 Variants from reference Self-training with Noisy Student improves ImageNet classification : efficientnet_l2 efficientnet_l2_475 Variants from reference EfficientNet-EdgeTPU: Creating Accelerator-Optimized Neural Networks with AutoML : efficientnet_edge_s efficientnet_edge_m efficientnet_edge_l Variants from reference Higher accuracy on vision models with EfficientNet-Lite : efficientnet_lite0 efficientnet_lite1 efficientnet_lite2 efficientnet_lite3 efficientnet_lite4","title":"EfficientNet"},{"location":"modules/backbones/#mobilenet","text":"Variants from reference MobileNetV2: Inverted Residuals and Linear Bottlenecks : mobilenet_v2 Variants from reference Searching for MobileNetV3 : mobilenetv3_small_w7d20 mobilenetv3_small_wd2 mobilenetv3_small_w3d4 mobilenetv3_small_w1 mobilenetv3_small_w5d4 mobilenetv3_large_w7d20 mobilenetv3_large_wd2 mobilenetv3_large_w3d4 mobilenetv3_large_w1 mobilenetv3_large_w5d4","title":"MobileNet"},{"location":"modules/backbones/#resnet","text":"Variants from reference Deep Residual Learning for Image Recognition : resnet18 resnet34 resnet50 resnet101 resnet152 Variants from reference Aggregated Residual Transformations for Deep Neural Networks : resnext50_32x4d resnext101_32x8d Variants from reference Wide Residual Networks : wide_resnet50_2 wide_resnet101_2","title":"ResNet"},{"location":"modules/backbones/#shufflenet","text":"Variants from reference ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design : shufflenetv2_x0.5 shufflenetv2_x1.0 shufflenetv2_x1.5 shufflenetv2_x2.0","title":"ShuffleNet"},{"location":"modules/backbones/#vgg","text":"Variants from reference Very Deep Convolutional Networks for Large-Scale Image Recognition : vgg11 vgg11_bn vgg13 vgg13_bn vgg16 vgg16_bn vgg19 vgg19_bn","title":"VGG"},{"location":"modules/builtin_dataset/","text":"Built-in Dataset \u00b6 In this module, we listed all the internally supported dataset to be used in Vortex. Part of dataset.train AND dataset.eval configurations in experiment file. Torchvision Dataset \u00b6 Several torchvision dataset is supported, they are listed below : MNIST FashionMNIST KMNIST EMNIST QMNIST ImageFolder CIFAR10 CIFAR100 SVHN STL10 To use these dataset, set the experiment file using the dataset identifier listed above, and pass the arguments like shown in their respective documentations in the args field. For example : dataset: { train: { dataset: CIFAR10, args: { root: external/datasets, train: True, download: True } }, eval: { dataset: CIFAR10, args: { root: external/datasets, train: False, download: True } } } **IMPORTANT NOTES : transform and target_transform arguments is not supported, augmentation will be supported by Vortex built-in augmentation mechanism specified in this step","title":"Built-in Dataset"},{"location":"modules/builtin_dataset/#built-in-dataset","text":"In this module, we listed all the internally supported dataset to be used in Vortex. Part of dataset.train AND dataset.eval configurations in experiment file.","title":"Built-in Dataset"},{"location":"modules/builtin_dataset/#torchvision-dataset","text":"Several torchvision dataset is supported, they are listed below : MNIST FashionMNIST KMNIST EMNIST QMNIST ImageFolder CIFAR10 CIFAR100 SVHN STL10 To use these dataset, set the experiment file using the dataset identifier listed above, and pass the arguments like shown in their respective documentations in the args field. For example : dataset: { train: { dataset: CIFAR10, args: { root: external/datasets, train: True, download: True } }, eval: { dataset: CIFAR10, args: { root: external/datasets, train: False, download: True } } } **IMPORTANT NOTES : transform and target_transform arguments is not supported, augmentation will be supported by Vortex built-in augmentation mechanism specified in this step","title":"Torchvision Dataset"},{"location":"modules/data_loader/","text":"Data Loader \u00b6 This section listed all available dataloader modules configurations. Part of dataset.dataloader configurations in experiment file. Pytorch Data Loader \u00b6 A standard Pytorch data loader. dataloader: { dataloader: DataLoader, args: { num_workers: 0, batch_size: 16, shuffle: True, }, } It\u2019s important to be noted that argument that expect function as its input is not supported, detailed arguments can be inspected in this page Common used arguments : num_workers (int) : how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process batch_size (int) : how many samples per batch to load (default: 1) shuffle (bool) : set to True to have the data reshuffled at every epoch (default: False).","title":"Data Loader"},{"location":"modules/data_loader/#data-loader","text":"This section listed all available dataloader modules configurations. Part of dataset.dataloader configurations in experiment file.","title":"Data Loader"},{"location":"modules/data_loader/#pytorch-data-loader","text":"A standard Pytorch data loader. dataloader: { dataloader: DataLoader, args: { num_workers: 0, batch_size: 16, shuffle: True, }, } It\u2019s important to be noted that argument that expect function as its input is not supported, detailed arguments can be inspected in this page Common used arguments : num_workers (int) : how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process batch_size (int) : how many samples per batch to load (default: 1) shuffle (bool) : set to True to have the data reshuffled at every epoch (default: False).","title":"Pytorch Data Loader"},{"location":"modules/exporter/","text":"Graph Exporter \u00b6 This section listed all available exporter configuration. Part of graph exporter section in experiment file. ONNX \u00b6 This module will produce ONNX IR from a trained Vortex model. Further reading : onnx.ai Currently, we provide export support for opset version 9,10, and 11. However it must be noted that not all models and backbones are supported to be converted to ONNX. The list of supported and not supported models and backbones can be found in the COMPATIBILITY REPORT on the repo : COMPATIBILITY REPORT Opset Version 9 COMPATIBILITY REPORT Opset Version 10 COMPATIBILITY REPORT Opset Version 11 E.g. : exporter: { module: onnx, args: { n_batch: 4, opset_version: 11, filename: somemodel_bs4 }, } Arguments : n_batch (int) : number of input batches that can be processed by this model at a time. An IR graph input batch size must be pre configured during export and cannot be modified afterwise opset_version (int) : selected ONNX opset version. For complete information check this link filename (str) : A new filename which will be given to the exported IR with .onnx suffix. If not given, the default is {experiment_name}.onnx Outputs : ONNX IR : ONNX file model (*.onnx) will be produced at experiment output directory Torchscript \u00b6 This module will produce Torchscript IR from a trained Vortex model. Further reading : torchscript E.g. : exporter: { module: torchscript, args: { n_batch: 4, filename: somemodel_bs4 }, } Arguments : n_batch (int) : number of input batches that can be processed by this model at a time. An IR graph input batch size must be pre configured during export and cannot be modified afterwise filename (str) : A new filename which will be given to the exported IR with .pt suffix. If not given, the default is {experiment_name}.pt Outputs : Torhscript IR : Torchscript file model (*.pt) will be produced at experiment output directory","title":"Graph Exporter"},{"location":"modules/exporter/#graph-exporter","text":"This section listed all available exporter configuration. Part of graph exporter section in experiment file.","title":"Graph Exporter"},{"location":"modules/exporter/#onnx","text":"This module will produce ONNX IR from a trained Vortex model. Further reading : onnx.ai Currently, we provide export support for opset version 9,10, and 11. However it must be noted that not all models and backbones are supported to be converted to ONNX. The list of supported and not supported models and backbones can be found in the COMPATIBILITY REPORT on the repo : COMPATIBILITY REPORT Opset Version 9 COMPATIBILITY REPORT Opset Version 10 COMPATIBILITY REPORT Opset Version 11 E.g. : exporter: { module: onnx, args: { n_batch: 4, opset_version: 11, filename: somemodel_bs4 }, } Arguments : n_batch (int) : number of input batches that can be processed by this model at a time. An IR graph input batch size must be pre configured during export and cannot be modified afterwise opset_version (int) : selected ONNX opset version. For complete information check this link filename (str) : A new filename which will be given to the exported IR with .onnx suffix. If not given, the default is {experiment_name}.onnx Outputs : ONNX IR : ONNX file model (*.onnx) will be produced at experiment output directory","title":"ONNX"},{"location":"modules/exporter/#torchscript","text":"This module will produce Torchscript IR from a trained Vortex model. Further reading : torchscript E.g. : exporter: { module: torchscript, args: { n_batch: 4, filename: somemodel_bs4 }, } Arguments : n_batch (int) : number of input batches that can be processed by this model at a time. An IR graph input batch size must be pre configured during export and cannot be modified afterwise filename (str) : A new filename which will be given to the exported IR with .pt suffix. If not given, the default is {experiment_name}.pt Outputs : Torhscript IR : Torchscript file model (*.pt) will be produced at experiment output directory","title":"Torchscript"},{"location":"modules/logging_provider/","text":"Logging Provider \u00b6 This section listed all available logging configuration. Part of logging section in experiment file. No Logging \u00b6 This configuration will disable any logging logging: None Comet-ML \u00b6 This configuration will enable experiment logging to comet.ml . Make sure that you have an account there. logging: { module: 'comet_ml', args: { api_key: HG65hasJHGFshuasg67, project_name: vortex-classification, workspace: hyperion-rg }, pytz_timezone: 'Asia/Jakarta' } Required Arguments : api_key : comet.ml user personal API key project_name : the experiment\u2019s project group workspace : the user\u2019s workspace Additional Arguments : See this link","title":"Logging Provider"},{"location":"modules/logging_provider/#logging-provider","text":"This section listed all available logging configuration. Part of logging section in experiment file.","title":"Logging Provider"},{"location":"modules/logging_provider/#no-logging","text":"This configuration will disable any logging logging: None","title":"No Logging"},{"location":"modules/logging_provider/#comet-ml","text":"This configuration will enable experiment logging to comet.ml . Make sure that you have an account there. logging: { module: 'comet_ml', args: { api_key: HG65hasJHGFshuasg67, project_name: vortex-classification, workspace: hyperion-rg }, pytz_timezone: 'Asia/Jakarta' } Required Arguments : api_key : comet.ml user personal API key project_name : the experiment\u2019s project group workspace : the user\u2019s workspace Additional Arguments : See this link","title":"Comet-ML"},{"location":"modules/models_zoo/","text":"Models Zoo \u00b6 This section listed all available model configuration. Part of model section in experiment file. Classification \u00b6 This task is about predicting a category for an input image. Softmax \u00b6 This classification model supports single-category multi-class classification objectives. The output will be a class with the highest prediction probability score (between 0 and 1) Reference : Softmax and probabilities E.g. : model: { name: softmax, network_args: { backbone: shufflenetv2_x1.0, n_classes: 10, pretrained_backbone: True, freeze_backbone: False },[ preprocess_args: { input_size: 224, input_normalization: { mean: [ 0.5, 0.5, 0.5 ], std: [ 0.5, 0.5, 0.5 ], scaler: 255 } }, loss_args: { loss: ce_loss, reduction: mean, additional_args: {} }, postprocess_args: {}, init_state_dict: somepath.pth } Arguments \u00b6 preprocess_args : see this section network_args : backbone (str) : backbones network name, supported backbones network is provided at backbones network section n_classes (int) : number of classes pretrained_backbone (bool) : using the provided pretrained backbone for weight initialization (transfer learning) freeze_backbone (bool) : freeze the backbone weight, if the backbone is frozen the weight in backbone model will not be updated during training loss_args : loss (str) : classification loss to be used, options : ce_loss - will use nn.NLLLoss from PyTorch, see this documentations for details. focal_loss - will use Vortex implementation of Focal loss, referenced from this repo reduction (str) : reduction of loss array, either sum or mean additional_args (dict) : additional arguments to be forwarded to the loss function, options: for ce_loss , see the documentation for additional arguments for focal_loss , see this link for further details, sub-arguments : gamma (float) : focusing parameter for modulating factor (1-p) alpha (List[float]) : per-class weighting array. For example, if you have a classification task with 3 class, you need to provide the weight in an array of size 3 = [0.1 ,0.2, 0.7] postprocess_args : you can leave this field with empty dict {} init_state_dict : see this section Outputs \u00b6 List of dictionary of np.ndarray pair, output key : class_label ( np.ndarray ) : array with size of 1, each column represents class label class_confidence ( np.ndarray ) : array with size of 1, each column represents class confidence NOTES : row orders are consistent : class_confidence[i] is associated with class_label[i] Detection \u00b6 This task is about predicting multiple objects location as pixel coordinates and its category from input image. Detection models\u2019 params and MAC/FLOPS comparison ( to compare which models is the lightest or heaviest ) can be found on this spreadsheet link FPN-SSD \u00b6 This implementation is basically RetinaNet without (not yet) focal loss implemented. DISCLAIMER : THIS MODEL IS NOT VERIFIED YET, MAY PRODUCE BAD RESULT Reference : Focal Loss for Dense Object Detection E.g. model: { name: FPNSSD, preprocess_args: { input_size: 512, input_normalization: { mean: [ 0.5, 0.5, 0.5 ], std: [ 0.5, 0.5, 0.5 ], scaler: 255 } }, network_args: { backbone: shufflenetv2_x1.0, n_classes: 20, pyramid_channels: 256, aspect_ratios: [ 1, 2., 3. ], pretrained_backbone: True, freeze_backbone: False }, loss_args: { neg_pos: 3, overlap_thresh: 0.5, }, postprocess_args: {}, init_state_dict: somepath.pth } Arguments \u00b6 preprocess_args : see this section network_args : backbone (str) : backbones network name, supported backbones network is provided at backbones network section n_classes (int) : number of classes pretrained_backbone (bool) : using the provided pretrained backbone for weight initialization (transfer learning) freeze_backbone (bool) : freeze the backbone weight, if the backbone is frozen the weight in backbone model will not be updated during training pyramid_channels (int) : number of channels for FPN top-down pyramid, default to 256, could be modified if necessary (e.g. for accuracy vs speed trade-off) (the lower the number, speed will increase)(the lower the number, speed will increase) aspect_ratios (list) : aspect ratio for anchor box loss_args : neg_pos (int) : negative (background) to positive (object) ratio for Hard Negative Mining overlap_thresh (float) : minimum iou threshold to be considered as positive during training postprocess_args : you can leave this field with empty dict {} init_state_dict : see this section Outputs \u00b6 List of dictionary of np.ndarray pair, output key : bounding_box ( np.ndarray ) : array with size n_detections * 4 , each column on x_min,y_min,x_max,y_max format class_label ( np.ndarray ) : array with size of n_detectionx * 1 , each column represents class label class_confidence ( np.ndarray ) : array with size of n_detections * 1 , each column represents class confidence NOTES : row orders are consistent : bounding_box[i] is associated with class_label[i] , etc.. Detection (with Landmarks/Keypoints) \u00b6 This task is about predicting multiple objects location as pixel coordinates, its category, and objects landmarks/keypoints in [x,y] coordinates from input image. Detection models\u2019 params and MAC/FLOPS comparison ( to compare which models is the lightest or heaviest ) can be found on this spreadsheet link RetinaFace \u00b6 This model perform 1 class object detection with 5 key points estimation, originally used for face detection and face landmarks prediction DISCLAIMER : THIS MODEL IS NOT VERIFIED YET, MAY PRODUCE BAD RESULT Reference : RetinaFace: Single-stage Dense Face Localisation in the Wild E.g. : model: { name: RetinaFace, preprocess_args: { input_size: 640, input_normalization: { mean: [ 0.5, 0.5, 0.5 ], std: [ 0.5, 0.5, 0.5 ] } }, network_args: { backbone: shufflenetv2_x1.0, pyramid_channels: 64, aspect_ratios: [ 1, 2., 3. ], pretrained_backbone: True, freeze_backbone: False }, loss_args: { neg_pos: 7, overlap_thresh: 0.35, cls: 2.0, box: 1.0, ldm: 1.0, }, postprocess_args: {}, init_state_dict: somepath.pth } Arguments \u00b6 preprocess_args : see this section network_args : backbone (str) : backbones network name, supported backbones network is provided at backbones network section pretrained_backbone (bool) : using the provided pretrained backbone for weight initialization (transfer learning) freeze_backbone (bool) : freeze the backbone weight, if the backbone is frozen the weight in backbone model will not be updated during training pyramid_channels (int) : number of channels for FPN top-down pyramid, default to 256, could be modified if necessary (e.g. for accuracy vs speed trade-off) (the lower the number, speed will increase)(the lower the number, speed will increase) aspect_ratios (list) : aspect ratio for anchor box loss_args : neg_pos (int) : negative (background) to positive (object) ratio for Hard Negative Mining overlap_thresh (float) : minimum iou threshold to be considered as positive during training cls (float) : weight for classification loss box (float) : weight for bounding box loss ldm (float) : weight for landmark regression loss postprocess_args : you can leave this field with empty dict {} init_state_dict : see this section Outputs \u00b6 List of dictionary of np.ndarray pair, output key : bounding_box ( np.ndarray ) : array with size n_detections * 4 , each column on x_min,y_min,x_max,y_max format class_label ( np.ndarray ) : array with size of n_detectionx * 1 , each column represents class label class_confidence ( np.ndarray ) : array with size of n_detections * 1 , each column represents class confidence landmarks ( np.ndarray ) : array with size of n_detections x 10 , each column represents landmark position with xy format, e.g. [p1x, p1y, p2x, p2y, \u2026] NOTES : row orders are consistent : bounding_box[i] is associated with class_label[i] , etc..","title":"Models Zoo"},{"location":"modules/models_zoo/#models-zoo","text":"This section listed all available model configuration. Part of model section in experiment file.","title":"Models Zoo"},{"location":"modules/models_zoo/#classification","text":"This task is about predicting a category for an input image.","title":"Classification"},{"location":"modules/models_zoo/#softmax","text":"This classification model supports single-category multi-class classification objectives. The output will be a class with the highest prediction probability score (between 0 and 1) Reference : Softmax and probabilities E.g. : model: { name: softmax, network_args: { backbone: shufflenetv2_x1.0, n_classes: 10, pretrained_backbone: True, freeze_backbone: False },[ preprocess_args: { input_size: 224, input_normalization: { mean: [ 0.5, 0.5, 0.5 ], std: [ 0.5, 0.5, 0.5 ], scaler: 255 } }, loss_args: { loss: ce_loss, reduction: mean, additional_args: {} }, postprocess_args: {}, init_state_dict: somepath.pth }","title":"Softmax"},{"location":"modules/models_zoo/#arguments","text":"preprocess_args : see this section network_args : backbone (str) : backbones network name, supported backbones network is provided at backbones network section n_classes (int) : number of classes pretrained_backbone (bool) : using the provided pretrained backbone for weight initialization (transfer learning) freeze_backbone (bool) : freeze the backbone weight, if the backbone is frozen the weight in backbone model will not be updated during training loss_args : loss (str) : classification loss to be used, options : ce_loss - will use nn.NLLLoss from PyTorch, see this documentations for details. focal_loss - will use Vortex implementation of Focal loss, referenced from this repo reduction (str) : reduction of loss array, either sum or mean additional_args (dict) : additional arguments to be forwarded to the loss function, options: for ce_loss , see the documentation for additional arguments for focal_loss , see this link for further details, sub-arguments : gamma (float) : focusing parameter for modulating factor (1-p) alpha (List[float]) : per-class weighting array. For example, if you have a classification task with 3 class, you need to provide the weight in an array of size 3 = [0.1 ,0.2, 0.7] postprocess_args : you can leave this field with empty dict {} init_state_dict : see this section","title":"Arguments"},{"location":"modules/models_zoo/#outputs","text":"List of dictionary of np.ndarray pair, output key : class_label ( np.ndarray ) : array with size of 1, each column represents class label class_confidence ( np.ndarray ) : array with size of 1, each column represents class confidence NOTES : row orders are consistent : class_confidence[i] is associated with class_label[i]","title":"Outputs"},{"location":"modules/models_zoo/#detection","text":"This task is about predicting multiple objects location as pixel coordinates and its category from input image. Detection models\u2019 params and MAC/FLOPS comparison ( to compare which models is the lightest or heaviest ) can be found on this spreadsheet link","title":"Detection"},{"location":"modules/models_zoo/#fpn-ssd","text":"This implementation is basically RetinaNet without (not yet) focal loss implemented. DISCLAIMER : THIS MODEL IS NOT VERIFIED YET, MAY PRODUCE BAD RESULT Reference : Focal Loss for Dense Object Detection E.g. model: { name: FPNSSD, preprocess_args: { input_size: 512, input_normalization: { mean: [ 0.5, 0.5, 0.5 ], std: [ 0.5, 0.5, 0.5 ], scaler: 255 } }, network_args: { backbone: shufflenetv2_x1.0, n_classes: 20, pyramid_channels: 256, aspect_ratios: [ 1, 2., 3. ], pretrained_backbone: True, freeze_backbone: False }, loss_args: { neg_pos: 3, overlap_thresh: 0.5, }, postprocess_args: {}, init_state_dict: somepath.pth }","title":"FPN-SSD"},{"location":"modules/models_zoo/#arguments_1","text":"preprocess_args : see this section network_args : backbone (str) : backbones network name, supported backbones network is provided at backbones network section n_classes (int) : number of classes pretrained_backbone (bool) : using the provided pretrained backbone for weight initialization (transfer learning) freeze_backbone (bool) : freeze the backbone weight, if the backbone is frozen the weight in backbone model will not be updated during training pyramid_channels (int) : number of channels for FPN top-down pyramid, default to 256, could be modified if necessary (e.g. for accuracy vs speed trade-off) (the lower the number, speed will increase)(the lower the number, speed will increase) aspect_ratios (list) : aspect ratio for anchor box loss_args : neg_pos (int) : negative (background) to positive (object) ratio for Hard Negative Mining overlap_thresh (float) : minimum iou threshold to be considered as positive during training postprocess_args : you can leave this field with empty dict {} init_state_dict : see this section","title":"Arguments"},{"location":"modules/models_zoo/#outputs_1","text":"List of dictionary of np.ndarray pair, output key : bounding_box ( np.ndarray ) : array with size n_detections * 4 , each column on x_min,y_min,x_max,y_max format class_label ( np.ndarray ) : array with size of n_detectionx * 1 , each column represents class label class_confidence ( np.ndarray ) : array with size of n_detections * 1 , each column represents class confidence NOTES : row orders are consistent : bounding_box[i] is associated with class_label[i] , etc..","title":"Outputs"},{"location":"modules/models_zoo/#detection-with-landmarkskeypoints","text":"This task is about predicting multiple objects location as pixel coordinates, its category, and objects landmarks/keypoints in [x,y] coordinates from input image. Detection models\u2019 params and MAC/FLOPS comparison ( to compare which models is the lightest or heaviest ) can be found on this spreadsheet link","title":"Detection (with Landmarks/Keypoints)"},{"location":"modules/models_zoo/#retinaface","text":"This model perform 1 class object detection with 5 key points estimation, originally used for face detection and face landmarks prediction DISCLAIMER : THIS MODEL IS NOT VERIFIED YET, MAY PRODUCE BAD RESULT Reference : RetinaFace: Single-stage Dense Face Localisation in the Wild E.g. : model: { name: RetinaFace, preprocess_args: { input_size: 640, input_normalization: { mean: [ 0.5, 0.5, 0.5 ], std: [ 0.5, 0.5, 0.5 ] } }, network_args: { backbone: shufflenetv2_x1.0, pyramid_channels: 64, aspect_ratios: [ 1, 2., 3. ], pretrained_backbone: True, freeze_backbone: False }, loss_args: { neg_pos: 7, overlap_thresh: 0.35, cls: 2.0, box: 1.0, ldm: 1.0, }, postprocess_args: {}, init_state_dict: somepath.pth }","title":"RetinaFace"},{"location":"modules/models_zoo/#arguments_2","text":"preprocess_args : see this section network_args : backbone (str) : backbones network name, supported backbones network is provided at backbones network section pretrained_backbone (bool) : using the provided pretrained backbone for weight initialization (transfer learning) freeze_backbone (bool) : freeze the backbone weight, if the backbone is frozen the weight in backbone model will not be updated during training pyramid_channels (int) : number of channels for FPN top-down pyramid, default to 256, could be modified if necessary (e.g. for accuracy vs speed trade-off) (the lower the number, speed will increase)(the lower the number, speed will increase) aspect_ratios (list) : aspect ratio for anchor box loss_args : neg_pos (int) : negative (background) to positive (object) ratio for Hard Negative Mining overlap_thresh (float) : minimum iou threshold to be considered as positive during training cls (float) : weight for classification loss box (float) : weight for bounding box loss ldm (float) : weight for landmark regression loss postprocess_args : you can leave this field with empty dict {} init_state_dict : see this section","title":"Arguments"},{"location":"modules/models_zoo/#outputs_2","text":"List of dictionary of np.ndarray pair, output key : bounding_box ( np.ndarray ) : array with size n_detections * 4 , each column on x_min,y_min,x_max,y_max format class_label ( np.ndarray ) : array with size of n_detectionx * 1 , each column represents class label class_confidence ( np.ndarray ) : array with size of n_detections * 1 , each column represents class confidence landmarks ( np.ndarray ) : array with size of n_detections x 10 , each column represents landmark position with xy format, e.g. [p1x, p1y, p2x, p2y, \u2026] NOTES : row orders are consistent : bounding_box[i] is associated with class_label[i] , etc..","title":"Outputs"},{"location":"modules/scheduler/","text":"Learning Rates Scheduler \u00b6 This section listed all available scheduler configuration. Part of trainer configurations in experiment file. Pytorch Scheduler \u00b6 The following example configuration uses the Pytorch StepLR scheduler. You can use the other scheduler by following similar fashion. List of Pytorch supported scheduler can be found in this link scheduler: { method: StepLR, args: { step_size: 50, gamma: 0.1 } }, CosineLR \u00b6 Implement Cosine decay scheduler with warm restarts Reference : SGDR: Stochastic Gradient Descent with Warm Restarts allennlp/cosine.py scheduler: { method: CosineLRScheduler, args: { t_initial: 200, t_mul: 1.0, lr_min: 0.00001, warmup_lr_init: 0.00001, warmup_t: 5, cycle_limit: 1, t_in_epochs: True, decay_rate: 0.1, } }, Arguments : t_initial (int) : the number of iterations (epochs) within the first cycle t_mul (float) : determines the number of iterations (epochs) in the i-th decay cycle, which is the length of the last cycle multiplied by t_mul . default : 1 lr_min (float) : minimum learning rate after decay. default : 0. warmup_lr_init (float) : starting learning rate on warmup stage. default : 0 warmup_t (int) : number of epoch of warmup stage. default : 0 cycle_limit (int) : number of cosine cycle. default : 0 t_in_epochs (bool) : if True, update learning rate per epoch, if not, update per step. default : True decay_rate (float) : learning rate decay rate. default : 1 TanhLR \u00b6 Implement Hyperbolic-Tangent decay with warm restarts Reference : Stochastic Gradient Descent with Hyperbolic-Tangent Decay on Classification scheduler: { method: TanhLRScheduler, args: { t_initial: 200, t_mul: 1.0, lb: -6., ub: 4., lr_min: 0.00001, warmup_lr_init: 0.00001, warmup_t: 5, cycle_limit: 1, t_in_epochs: True, decay_rate: 0.1, } } Arguments : t_initial (int) : the number of iterations (epochs) within the first cycle t_mul (float) : determines the number of iterations (epochs) in the i-th decay cycle, which is the length of the last cycle multiplied by t_mul . default : 1 lb (float) : tanh function lower bound value ub (float) : tanh function upper bound value lr_min (float) : Minimum learning rate after decay. default : 0. warmup_lr_init (float) : starting learning rate on warmup stage. default : 0 warmup_t (int) : number of epoch of warmup stage. default : 0 cycle_limit (int) : number of cosine cycle. default : 0 t_in_epochs (bool) : if True, update learning rate per epoch, if not, update per step. default : True decay_rate (float) : learning rate decay rate. default : 1 StepLRWithBurnIn \u00b6 Implement StepLR scheduler with burn in (warm start), adapted from YOLOv3 training method Reference : DeNA/PyTorch_YOLOv3: Implementation of YOLOv3 in PyTorch scheduler: { method: StepLRWithBurnIn, args: { burn_in: 5, steps: [180,190], scales: [.1,.1], last_epoch: -1 } } Arguments : burn_in (int) : number of epochs for warm up steps (list) : list of epoch when the learning rate will be reduced, e.g. [180,190] --> learning rate will be reduced on epoch 180 and epoch 190 scales (list) : scale of the reduced learning rate, e.g. [0.1,0.1] --> e.g. initial lr == 0.01 , on epoch 180 will be reduced to 0.1 * 0.01 = 0.001 and on epoch 190 will be reduced to 0.1 * 0.001 = 0.0001 last_epoch (int) : last epoch number. default : -1","title":"Learning Rates Scheduler"},{"location":"modules/scheduler/#learning-rates-scheduler","text":"This section listed all available scheduler configuration. Part of trainer configurations in experiment file.","title":"Learning Rates Scheduler"},{"location":"modules/scheduler/#pytorch-scheduler","text":"The following example configuration uses the Pytorch StepLR scheduler. You can use the other scheduler by following similar fashion. List of Pytorch supported scheduler can be found in this link scheduler: { method: StepLR, args: { step_size: 50, gamma: 0.1 } },","title":"Pytorch Scheduler"},{"location":"modules/scheduler/#cosinelr","text":"Implement Cosine decay scheduler with warm restarts Reference : SGDR: Stochastic Gradient Descent with Warm Restarts allennlp/cosine.py scheduler: { method: CosineLRScheduler, args: { t_initial: 200, t_mul: 1.0, lr_min: 0.00001, warmup_lr_init: 0.00001, warmup_t: 5, cycle_limit: 1, t_in_epochs: True, decay_rate: 0.1, } }, Arguments : t_initial (int) : the number of iterations (epochs) within the first cycle t_mul (float) : determines the number of iterations (epochs) in the i-th decay cycle, which is the length of the last cycle multiplied by t_mul . default : 1 lr_min (float) : minimum learning rate after decay. default : 0. warmup_lr_init (float) : starting learning rate on warmup stage. default : 0 warmup_t (int) : number of epoch of warmup stage. default : 0 cycle_limit (int) : number of cosine cycle. default : 0 t_in_epochs (bool) : if True, update learning rate per epoch, if not, update per step. default : True decay_rate (float) : learning rate decay rate. default : 1","title":"CosineLR"},{"location":"modules/scheduler/#tanhlr","text":"Implement Hyperbolic-Tangent decay with warm restarts Reference : Stochastic Gradient Descent with Hyperbolic-Tangent Decay on Classification scheduler: { method: TanhLRScheduler, args: { t_initial: 200, t_mul: 1.0, lb: -6., ub: 4., lr_min: 0.00001, warmup_lr_init: 0.00001, warmup_t: 5, cycle_limit: 1, t_in_epochs: True, decay_rate: 0.1, } } Arguments : t_initial (int) : the number of iterations (epochs) within the first cycle t_mul (float) : determines the number of iterations (epochs) in the i-th decay cycle, which is the length of the last cycle multiplied by t_mul . default : 1 lb (float) : tanh function lower bound value ub (float) : tanh function upper bound value lr_min (float) : Minimum learning rate after decay. default : 0. warmup_lr_init (float) : starting learning rate on warmup stage. default : 0 warmup_t (int) : number of epoch of warmup stage. default : 0 cycle_limit (int) : number of cosine cycle. default : 0 t_in_epochs (bool) : if True, update learning rate per epoch, if not, update per step. default : True decay_rate (float) : learning rate decay rate. default : 1","title":"TanhLR"},{"location":"modules/scheduler/#steplrwithburnin","text":"Implement StepLR scheduler with burn in (warm start), adapted from YOLOv3 training method Reference : DeNA/PyTorch_YOLOv3: Implementation of YOLOv3 in PyTorch scheduler: { method: StepLRWithBurnIn, args: { burn_in: 5, steps: [180,190], scales: [.1,.1], last_epoch: -1 } } Arguments : burn_in (int) : number of epochs for warm up steps (list) : list of epoch when the learning rate will be reduced, e.g. [180,190] --> learning rate will be reduced on epoch 180 and epoch 190 scales (list) : scale of the reduced learning rate, e.g. [0.1,0.1] --> e.g. initial lr == 0.01 , on epoch 180 will be reduced to 0.1 * 0.01 = 0.001 and on epoch 190 will be reduced to 0.1 * 0.001 = 0.0001 last_epoch (int) : last epoch number. default : -1","title":"StepLRWithBurnIn"},{"location":"modules/train_driver/","text":"Training Driver \u00b6 This section listed all available driver configuration. Part of trainer configurations in experiment file. Default Trainer \u00b6 The basic training driver. A model will be forwarded batched inputs, loss will be calculated and back propagated. This trainer supports gradient accumulation in which backpropagation gradient can be bigger than batch size. For example : Let's say batch_size in dataloader is 16 , accumulation_step is 4 . The gradient calculation will come from batch_size * accumulation_step which is equivalent to 16*4=64 . Thus even if with limited resource, training with simulated larger batch size is possible. E.g. : driver: { module: DefaultTrainer, args: { accumulation_step: 4, } } Arguments : accumulation_step (int) : number of iterations before gradient is back propagated","title":"Training Driver"},{"location":"modules/train_driver/#training-driver","text":"This section listed all available driver configuration. Part of trainer configurations in experiment file.","title":"Training Driver"},{"location":"modules/train_driver/#default-trainer","text":"The basic training driver. A model will be forwarded batched inputs, loss will be calculated and back propagated. This trainer supports gradient accumulation in which backpropagation gradient can be bigger than batch size. For example : Let's say batch_size in dataloader is 16 , accumulation_step is 4 . The gradient calculation will come from batch_size * accumulation_step which is equivalent to 16*4=64 . Thus even if with limited resource, training with simulated larger batch size is possible. E.g. : driver: { module: DefaultTrainer, args: { accumulation_step: 4, } } Arguments : accumulation_step (int) : number of iterations before gradient is back propagated","title":"Default Trainer"},{"location":"user-guides/dataset_integration/","text":"Dataset Integration \u00b6 To integrate external datasets into vortex, you need to follow several standards. This section will describe those standards. Directory Structure Standards \u00b6 The dataset directory must be placed under directory external/datasets . By default, Vortex will search this folder in the current working directory. However, should you place this directory elsewhere, you can set the environment variable VORTEX_DATASET_ROOT so Vortex can find it. Example : export VORTEX_DATASET_ROOT = /home/alvinprayuda /* Means that the dataset is in /home/alvinprayuda/external/datasets */ So, the directory structure will looked like this external/ datasets/ {dataset-directory} Each dataset directory must provide a python module /utils/dataset.py which will act as interface {dataset-directory}/ utils/ dataset.py Python Module Standards \u00b6 The python module can have several dataset interface classes. However it must be noted that a class represents a dataset. E.g. : class VOC0712DetectionDataset : def __init__(self): \"\"\" Implement something here \"\"\" pass All available dataset classes in the utils/dataset.py module must be listed in a variable named supported_datasets . E.g. : supported_dataset = [ 'VOC0712DetectionDataset' ] class VOC0712DetectionDataset : def __init__(self): \"\"\" Implement something here \"\"\" pass The utils/dataset.py interface python module must implement a function create_dataset . This function will receive args from the experiment file. E.g. : class VOC0712DetectionDataset : def __init__(self,*args,**kwargs): \"\"\" Implement something here \"\"\" pass def create_dataset(*args, **kwargs) : return VOC0712DetectionDataset(*args, **kwargs) Dataset Class Standards \u00b6 Each dataset class must implement several mandatory method and attributes: Method __getitem__ and __len__ similar to Pytorch dataset implementation : __len__ function must return the number (int) of dataset e.g. the number of images. E.g. : class VOC0712DetectionDataset : def __init__(self,*args,**kwargs): self.images = ['images/image1.jpg','images/image2.jpg','images/image3.jpg'] def __len__(self): return len(self.images) __getitem__ function returned value must be a tuple of image path (str) and its annotations (numpy array). E.g. Classification task, if you choose not to use torchvision's ImageFolder , the returned target's array dimension is 1 class ImageClassificationDataset : def __init__(self,*args,**kwargs): self.images = ['images/image1.jpg','images/image2.jpg','images/image3.jpg'] self.labels = [[0],[2],[1]] def __getitem__(self, index): img_path = self.images[index] target = np.array(self.labels[index]) return img_path, target dataset = ImageClassificationDataset() print(dataset[0][0]) \"\"\" 'images/image1.jpg' \"\"\" print(dataset[0][1]) \"\"\" array([0], dtype=float32) \"\"\" Detection task, the returned target's array dimension is 2 import numpy as np class VOC0712DetectionDataset : def __init__(self,*args,**kwargs): self.images = ['images/image1.jpg','images/image2.jpg','images/image3.jpg'] self.labels = [ [[0. , 0.5 , 0.5 , 0.3 , 0.2],[0. , 0.2 , 0.3 , 0.4 , 0.5]], [[0. , 0.1 , 0.2 , 0.3 , 0.4]], [[1. , 0.7 , 0.5 , 0.2 , 0.3],[2. , 0.4 , 0.4 , 0.3 , 0.3]], ] def __getitem__(self, index): img_path = self.images[index] target = np.array(self.labels[index]) return img_path, target dataset = VOC0712DetectionDataset() print(dataset[0][0]) \"\"\" 'images/image1.jpg' \"\"\" print(dataset[0][1]) \"\"\" array([[0. , 0.5, 0.5, 0.3, 0.2], [0. , 0.2, 0.3, 0.4, 0.5]], dtype=float32) \"\"\" Attribute self.class_names and self.data_format self.class_names contains information about class index to string . The value must be a list with string members which the sequence corresponds to its integer class index. The returned class labels in dataset's target must correspond to this list. E.g. class ImageClassificationDataset : def __init__(self,*args,**kwargs): self.images = ['images/image1.jpg','images/image2.jpg','images/image3.jpg'] self.labels = [[0],[2],[1]] self.class_names = ['cat','dog','bird'] def __getitem__(self, index): img_path = self.images[index] target = np.array(self.labels[index]) return img_path, target dataset = ImageClassificationDataset() print(dataset[0][0]) \"\"\" 'images/image1.jpg' \"\"\" print(dataset[0][1]) \"\"\" array([0], dtype=float32) \"\"\" class_label = dataset[0][1] class_label_string_name = dataset.class_names[class_label[0]] print(class_label_string_name) \"\"\" 'cat' This means that class_label = 0 correspond to string 'cat' in the self.class_names \"\"\" self.data_format which explains the format of dataset's target array and will be used to extract information from it. This attribute is specifically different between different tasks. Vortex utilizes numpy.take to slice the information from the dataset's target array. E.g. \"\"\" Example self.data_format = { 'bounding_box' : { 'indices' : [0, 1, 2, 3], 'axis' : 1 }, 'class_label' : { 'indices' : [4], 'axis' : 1 } } 'indices' : [0, 1, 2, 3] -> indicate x,y,w,h index of bounding box notation from labels array 'axis' : 1 -> specify the axis in which we slice the labels array --Example 'x' 'y' 'w' 'h' 'class' target_array=np.array([[ 0.75 , 0.6 , 0.1 , 0.2 , 8 ] [ 0.5 , 0.22 , 0.3 , 0.4 , 7 ]]) Using above data format we can slice the array to get only the bounding boxes coords bbox_array = np.array([[ 0.75 , 0.6 , 0.1 , 0.28] [ 0.5 , 0.22 , 0.3 , 0.4]]) class_array = np.array([[8] [7]]) \"\"\" Classification Task Class Label Data Format \"\"\" Class label data format \"\"\" self.data_format = {'class_label' : None} \"\"\" Because the annotations array size only 1, no need to specify indices and axis However, `self.data_format` is still mandatory \"\"\" Detection Task Class Label Data Format \"\"\" Class label data format \"\"\" \"\"\" Option 1 Indicate a single class notation for object detection \"\"\" self.data_format = {'class_label' : None} \"\"\" Option 2 Indicate a single-category multi-class notation \"\"\" self.data_format = { 'class_label' : { 'indices' : [4], 'axis' : 1 } } \"\"\" Option 3 (FUTURE-PLAN,NOT SUPPORTED YET) Indicate a multi-category multi-class notation \"\"\" self.data_format = { 'class_label' : { 'indices' : [4,5,6], 'axis' : 1 } } \"\"\" Option 4 (FUTURE-PLAN,NOT SUPPORTED YET) Indicate a multi-category multi-class notation with sequential long indexes \"\"\" self.data_format = { 'class_label' : { 'indices' : { 'start' : 4, 'end' : 6 }, 'axis' : 1 } } \"\"\" Explanation 'indices' with dict format and keys 'start' and 'end' will be converted to indices sequence internally \"\"\" Bounding Box Data Format It must be noted that VORTEX utilize [x,y,w,h] bounding box format in a normalized style (range 0 - 1 , [x] and [w] are normalized to image\u2019s width, whereas [y] and [h] normalized to image\u2019s height ) \"\"\" Bounding box data format \"\"\" self.data_format = { 'bounding_box' : { 'indices' : [0, 1, 2, 3], 'axis' : 1 }, } Landmarks (Key Points) Data Format (OPTIONAL) This data format is 'optional' in the sense that not all detection models that support landmark (key points) prediction. Thus if you want to utilize model that predict landmarks, such as RetinaFace, this data format is mandatory Landmarks annotation is presented as a 1-dimensional array which has an even length . E.g. [ x1,y1, x2,y2, x3,y3, x4,y4, x5,y5 ] The given example means that we have 5 landmarks with the coordinates of (x1,y1),(x2,y2),(x3,y3),(x4,y4), and (x5,y5) and also in normalized style (range 0 - 1 , [x] are normalized to image\u2019s width, whereas [y] normalized to image\u2019s height ) \"\"\" Landmarks data format \"\"\" \"\"\" Option 1 Standard implementation \"\"\" self.data_format = { 'landmarks' : { 'indices' : [7,8,9,10,11,12,13,14,15,16], 'axis' : 1 } } \"\"\" Option 2 With asymmetric keypoint declaration \"\"\" self.data_format = { 'landmarks' : { 'indices' : [7,8,9,10,11,12,13,14,15,16], 'asymm_pairs' : [[0,1],[3,4]], 'axis' : 1 } } \"\"\" Option 3 Implementation with long sequences \"\"\" self.data_format = { 'landmarks' : { 'indices' : { 'start' : 7, 'end' : 16 } 'asymm_pairs' : [[0,1],[3,4]], 'axis' : 1 } } \"\"\" Explanation 'indices' : [7,8,9,10,11,12,13,14,15,16] or 'indices' : { 'start' : 7, 'end' : 16 } The implementatiom above indicates a sequence of x,y coordinates (e.g index 7,9,11,13,15 -> x coordinates , index 8,10,12,14,16 -> y coordinates) Indices length must be even number 'asymm_pairs' : [[0,1],[3,4]] Indicates asymmetric key points which can be affected by vertical/ horizontal-flip data augmentation For example : Internally, indices [7,8,9,10,11,12,13,14,15,16] will be converted to [(7,8),(9,10),(11,12),(13,14),(15,16)] which means that the key points indexes are : keypoint 0 -> (7,8) keypoint 1 -> (9,10) keypoint 2 -> (11,12) keypoint 3 -> (13,14) keypoint 4 -> (15,16) In this example, we follow 5 facial landmarks example in which left and right landmarks sequence is crucial keypoint 0 -> (7,8) -> left eye keypoint 1 -> (9,10) -> right eye keypoint 2 -> (11,12) -> nose keypoint 3 -> (13,14) -> left mouth keypoint 4 -> (15,16) -> right mouth To handle this, the data format should specify which key points index have asymmetric relation, in this case keypoint 0-1 and keypoint 3-4, so we annotate them as in a list as [[0,1],[3,4]] \"\"\"","title":"Dataset Integration"},{"location":"user-guides/dataset_integration/#dataset-integration","text":"To integrate external datasets into vortex, you need to follow several standards. This section will describe those standards.","title":"Dataset Integration"},{"location":"user-guides/dataset_integration/#directory-structure-standards","text":"The dataset directory must be placed under directory external/datasets . By default, Vortex will search this folder in the current working directory. However, should you place this directory elsewhere, you can set the environment variable VORTEX_DATASET_ROOT so Vortex can find it. Example : export VORTEX_DATASET_ROOT = /home/alvinprayuda /* Means that the dataset is in /home/alvinprayuda/external/datasets */ So, the directory structure will looked like this external/ datasets/ {dataset-directory} Each dataset directory must provide a python module /utils/dataset.py which will act as interface {dataset-directory}/ utils/ dataset.py","title":"Directory Structure Standards"},{"location":"user-guides/dataset_integration/#python-module-standards","text":"The python module can have several dataset interface classes. However it must be noted that a class represents a dataset. E.g. : class VOC0712DetectionDataset : def __init__(self): \"\"\" Implement something here \"\"\" pass All available dataset classes in the utils/dataset.py module must be listed in a variable named supported_datasets . E.g. : supported_dataset = [ 'VOC0712DetectionDataset' ] class VOC0712DetectionDataset : def __init__(self): \"\"\" Implement something here \"\"\" pass The utils/dataset.py interface python module must implement a function create_dataset . This function will receive args from the experiment file. E.g. : class VOC0712DetectionDataset : def __init__(self,*args,**kwargs): \"\"\" Implement something here \"\"\" pass def create_dataset(*args, **kwargs) : return VOC0712DetectionDataset(*args, **kwargs)","title":"Python Module Standards"},{"location":"user-guides/dataset_integration/#dataset-class-standards","text":"Each dataset class must implement several mandatory method and attributes: Method __getitem__ and __len__ similar to Pytorch dataset implementation : __len__ function must return the number (int) of dataset e.g. the number of images. E.g. : class VOC0712DetectionDataset : def __init__(self,*args,**kwargs): self.images = ['images/image1.jpg','images/image2.jpg','images/image3.jpg'] def __len__(self): return len(self.images) __getitem__ function returned value must be a tuple of image path (str) and its annotations (numpy array). E.g. Classification task, if you choose not to use torchvision's ImageFolder , the returned target's array dimension is 1 class ImageClassificationDataset : def __init__(self,*args,**kwargs): self.images = ['images/image1.jpg','images/image2.jpg','images/image3.jpg'] self.labels = [[0],[2],[1]] def __getitem__(self, index): img_path = self.images[index] target = np.array(self.labels[index]) return img_path, target dataset = ImageClassificationDataset() print(dataset[0][0]) \"\"\" 'images/image1.jpg' \"\"\" print(dataset[0][1]) \"\"\" array([0], dtype=float32) \"\"\" Detection task, the returned target's array dimension is 2 import numpy as np class VOC0712DetectionDataset : def __init__(self,*args,**kwargs): self.images = ['images/image1.jpg','images/image2.jpg','images/image3.jpg'] self.labels = [ [[0. , 0.5 , 0.5 , 0.3 , 0.2],[0. , 0.2 , 0.3 , 0.4 , 0.5]], [[0. , 0.1 , 0.2 , 0.3 , 0.4]], [[1. , 0.7 , 0.5 , 0.2 , 0.3],[2. , 0.4 , 0.4 , 0.3 , 0.3]], ] def __getitem__(self, index): img_path = self.images[index] target = np.array(self.labels[index]) return img_path, target dataset = VOC0712DetectionDataset() print(dataset[0][0]) \"\"\" 'images/image1.jpg' \"\"\" print(dataset[0][1]) \"\"\" array([[0. , 0.5, 0.5, 0.3, 0.2], [0. , 0.2, 0.3, 0.4, 0.5]], dtype=float32) \"\"\" Attribute self.class_names and self.data_format self.class_names contains information about class index to string . The value must be a list with string members which the sequence corresponds to its integer class index. The returned class labels in dataset's target must correspond to this list. E.g. class ImageClassificationDataset : def __init__(self,*args,**kwargs): self.images = ['images/image1.jpg','images/image2.jpg','images/image3.jpg'] self.labels = [[0],[2],[1]] self.class_names = ['cat','dog','bird'] def __getitem__(self, index): img_path = self.images[index] target = np.array(self.labels[index]) return img_path, target dataset = ImageClassificationDataset() print(dataset[0][0]) \"\"\" 'images/image1.jpg' \"\"\" print(dataset[0][1]) \"\"\" array([0], dtype=float32) \"\"\" class_label = dataset[0][1] class_label_string_name = dataset.class_names[class_label[0]] print(class_label_string_name) \"\"\" 'cat' This means that class_label = 0 correspond to string 'cat' in the self.class_names \"\"\" self.data_format which explains the format of dataset's target array and will be used to extract information from it. This attribute is specifically different between different tasks. Vortex utilizes numpy.take to slice the information from the dataset's target array. E.g. \"\"\" Example self.data_format = { 'bounding_box' : { 'indices' : [0, 1, 2, 3], 'axis' : 1 }, 'class_label' : { 'indices' : [4], 'axis' : 1 } } 'indices' : [0, 1, 2, 3] -> indicate x,y,w,h index of bounding box notation from labels array 'axis' : 1 -> specify the axis in which we slice the labels array --Example 'x' 'y' 'w' 'h' 'class' target_array=np.array([[ 0.75 , 0.6 , 0.1 , 0.2 , 8 ] [ 0.5 , 0.22 , 0.3 , 0.4 , 7 ]]) Using above data format we can slice the array to get only the bounding boxes coords bbox_array = np.array([[ 0.75 , 0.6 , 0.1 , 0.28] [ 0.5 , 0.22 , 0.3 , 0.4]]) class_array = np.array([[8] [7]]) \"\"\" Classification Task Class Label Data Format \"\"\" Class label data format \"\"\" self.data_format = {'class_label' : None} \"\"\" Because the annotations array size only 1, no need to specify indices and axis However, `self.data_format` is still mandatory \"\"\" Detection Task Class Label Data Format \"\"\" Class label data format \"\"\" \"\"\" Option 1 Indicate a single class notation for object detection \"\"\" self.data_format = {'class_label' : None} \"\"\" Option 2 Indicate a single-category multi-class notation \"\"\" self.data_format = { 'class_label' : { 'indices' : [4], 'axis' : 1 } } \"\"\" Option 3 (FUTURE-PLAN,NOT SUPPORTED YET) Indicate a multi-category multi-class notation \"\"\" self.data_format = { 'class_label' : { 'indices' : [4,5,6], 'axis' : 1 } } \"\"\" Option 4 (FUTURE-PLAN,NOT SUPPORTED YET) Indicate a multi-category multi-class notation with sequential long indexes \"\"\" self.data_format = { 'class_label' : { 'indices' : { 'start' : 4, 'end' : 6 }, 'axis' : 1 } } \"\"\" Explanation 'indices' with dict format and keys 'start' and 'end' will be converted to indices sequence internally \"\"\" Bounding Box Data Format It must be noted that VORTEX utilize [x,y,w,h] bounding box format in a normalized style (range 0 - 1 , [x] and [w] are normalized to image\u2019s width, whereas [y] and [h] normalized to image\u2019s height ) \"\"\" Bounding box data format \"\"\" self.data_format = { 'bounding_box' : { 'indices' : [0, 1, 2, 3], 'axis' : 1 }, } Landmarks (Key Points) Data Format (OPTIONAL) This data format is 'optional' in the sense that not all detection models that support landmark (key points) prediction. Thus if you want to utilize model that predict landmarks, such as RetinaFace, this data format is mandatory Landmarks annotation is presented as a 1-dimensional array which has an even length . E.g. [ x1,y1, x2,y2, x3,y3, x4,y4, x5,y5 ] The given example means that we have 5 landmarks with the coordinates of (x1,y1),(x2,y2),(x3,y3),(x4,y4), and (x5,y5) and also in normalized style (range 0 - 1 , [x] are normalized to image\u2019s width, whereas [y] normalized to image\u2019s height ) \"\"\" Landmarks data format \"\"\" \"\"\" Option 1 Standard implementation \"\"\" self.data_format = { 'landmarks' : { 'indices' : [7,8,9,10,11,12,13,14,15,16], 'axis' : 1 } } \"\"\" Option 2 With asymmetric keypoint declaration \"\"\" self.data_format = { 'landmarks' : { 'indices' : [7,8,9,10,11,12,13,14,15,16], 'asymm_pairs' : [[0,1],[3,4]], 'axis' : 1 } } \"\"\" Option 3 Implementation with long sequences \"\"\" self.data_format = { 'landmarks' : { 'indices' : { 'start' : 7, 'end' : 16 } 'asymm_pairs' : [[0,1],[3,4]], 'axis' : 1 } } \"\"\" Explanation 'indices' : [7,8,9,10,11,12,13,14,15,16] or 'indices' : { 'start' : 7, 'end' : 16 } The implementatiom above indicates a sequence of x,y coordinates (e.g index 7,9,11,13,15 -> x coordinates , index 8,10,12,14,16 -> y coordinates) Indices length must be even number 'asymm_pairs' : [[0,1],[3,4]] Indicates asymmetric key points which can be affected by vertical/ horizontal-flip data augmentation For example : Internally, indices [7,8,9,10,11,12,13,14,15,16] will be converted to [(7,8),(9,10),(11,12),(13,14),(15,16)] which means that the key points indexes are : keypoint 0 -> (7,8) keypoint 1 -> (9,10) keypoint 2 -> (11,12) keypoint 3 -> (13,14) keypoint 4 -> (15,16) In this example, we follow 5 facial landmarks example in which left and right landmarks sequence is crucial keypoint 0 -> (7,8) -> left eye keypoint 1 -> (9,10) -> right eye keypoint 2 -> (11,12) -> nose keypoint 3 -> (13,14) -> left mouth keypoint 4 -> (15,16) -> right mouth To handle this, the data format should specify which key points index have asymmetric relation, in this case keypoint 0-1 and keypoint 3-4, so we annotate them as in a list as [[0,1],[3,4]] \"\"\"","title":"Dataset Class Standards"},{"location":"user-guides/experiment_file_config/","text":"Experiment File Configuration \u00b6 A YAML experiment file is needed to navigate all of Vortex pipelines. This experiment file will contain and track all configurations during all pipelines and can be a single source of information on how a model is developed. Several examples of experiment file can be inspected on this link . In this guide, we will cover all available sections that can be configured. All available configurations is listed below : Experiment Name \u00b6 Flagged with experiment_name key (str) in the experiment file. This field acts as an experiment identifier and is related to the experiment output directory where the trained model,reports, backups,and etc. will be dumped. E.g. : experiment_name: shufflenetv2x100_softmax_cifar10 Logging \u00b6 Flagged with logging key (dict or str( None )) in the experiment file. This field denotes logging providers that can be used for experiment logging, such as tensorboard, comet-ml, etc. The supported logging provider listed in the logging provider section . E.g. : logging: { module: comet_ml, args: { api_key: HGhusyd76hsGiSbt27688, project_name: vortex-classification, workspace: hyperion-rg }, pytz_timezone: Asia/Jakarta } Arguments : module (str) : denotes a specific logging provider module args (dict) : the corresponding arguments for selected module pytz_timezone (str) : the setting of the recorded experiment run timezone in Vortex local system. All timezone settings can be found in this link Output Directory \u00b6 Flagged with output_directory key (str) in the experiment file. This configuration set the location where experiment's output directory will be created. E.g. : output_directory: experiments/outputs Reproducibility and cuDNN auto-tune (OPTIONAL) \u00b6 Flagged with seed key (dict) in the experiment file. This configuration setting can be set if the user wants a model training experiment reproducibility. However as noted in this reference , completely reproducible results are not guaranteed across PyTorch releases, individual commits or different platforms, even with identical seeds. However we still can make the computation deterministic, to produce similar results. For further information, you can check this link . E.g. : seed: { torch: 0, numpy: 0, cudnn: { deterministic: True, benchmark: False, } } Arguments : torch (int) : set the Pytorch seed numpy (int) : set the Numpy seed cudnn (dict) : cuDNN configurations, sub-arguments : deterministic (bool) : set the Pytorch computation to be deterministic benchmark (bool) : set the cuDNN auto-tuner The cudnn args only need to be specified if the user trains using NVIDIA GPU (CuDNN backend). Additionally, by using this seed configuration you also use cuDNN auto-tune capability by using the following configuration seed: { cudnn: { benchmark: True, } } Dataset \u00b6 Flagged with dataset key (dict) in the experiment file. This is the configurations of the dataset, augmentations, and dataloader which will be used in the training. E.g. : dataset: { train: { dataset: VOC0712DetectionDataset, args: { image_set: train, }, augmentations: [ { module: albumentations, args: { transforms: [ { transform: HorizontalFlip, args: { p: 0.5}}, ], bbox_params: { min_visibility: 0.0, min_area: 0.0 }, visual_debug: False } }, # Example if you want to add another augmentation module # { # module: imgaug, # NOTES : `imgaug` module is not implemented yet, just example # args: { # transforms: [] # } # } ] }, eval: { dataset: VOC0712DetectionDataset, args: { image_set: val } }, dataloader: { dataloader: DataLoader, args: { num_workers: 0, batch_size: 16, shuffle: True, }, } } Arguments : train AND eval (str) ( eval is Optional): denotes the configuration of training and validation dataset respectively, if eval is not provided, in-loop validation process will be skipped. eval is mandatory for validation pipeline. Sub-arguments : dataset (str) : the dataset class names which will be used, mentioned in getting started section step 1. args (dict) : the corresponding arguments to the respective dataset class initialization augmentations (list[dict]) ( train only) : the augmentation configurations for training dataset. Augmentation modules provided in the list will be executed sequentially. sub-arguments (list members as dict) : module (str) : selected augmentation module, see augmentation module section for supported augmentation modules args (dict) : the corresponding arguments for selected module dataloader (dict) : denotes the configuration of the dataset iterator dataloader (str) : specify the dataloader module which will be used, supported data loader modules is provided at data loader module section args (dict) : the corresponding arguments for selected dataloader Trainer \u00b6 Flagged with trainer key (dict) in the experiment file. This configuration set how we train, validate on training, and several other configurations related to training iterations. E.g. : trainer: { optimizer: { method: SGD, args: { lr: 0.001, momentum: 0.9, weight_decay: 0.0005 } }, scheduler: { method: CosineLRScheduler, args: { t_initial: 200, t_mul: 1.0, lr_min: 0.00001, warmup_lr_init: 0.00001, warmup_t: 3, cycle_limit: 1, t_in_epochs: True, decay_rate: 0.1 } }, validation: { args: { score_threshold: 0.9, iou_threshold: 0.2, }, val_epoch: 10 }, epoch: 200, save_epoch: 1, device: 'cuda:0', driver: { module: DefaultTrainer, args: { accumulation_step: 4, } } } Arguments : optimizer (dict) : configuration for optimization algorithm. Sub-arguments : method (str) : optimization method identifier, currently support all optimizers supported by Pytorch listed in this link args (dict) : the corresponding arguments to the respective optimizer method scheduler (dict) : methods to adjust the learning rate based on the number of epochs method (str) : scheduler method identifier, supported scheduler methods is provided at scheduler section args (dict) : the corresponding arguments to the respective scheduler method validation (dict) (Optional) : additional parameter for validation process inside the training loop, if not provided in-loop validation process will be skipped. Sub-arguments : args (dict) : additional arguments for model postprocessing. Dependent on the described model's task. For detection : score_threshold (float) : threshold applied to the model\u2019s predicted object confidence score. Only objects with prediction scores higher than this threshold will be considered true objects, else considered as background. iou_threshold (float) : threshold for non-maxima suppression (NMS) intersection over union (IoU) For classification : No additional arguments for this task, you can leave args with empty dict {} val_epoch (int) : periodic number of epoch when the validation process will be executed in the training loop epoch (int) : number of dataset iteration (epoch) being done on the training dataset. 1 epoch is 1 dataset iteration save_epoch (int) : number of epoch before a model checkpoint being saved for backup device (str) : set the training device, whether using CPU : cpu or cuda GPU : cuda . you can add :{i} to cuda to point for specific GPU index {i} , E.g. cuda:0 for GPU index 0, cuda:1 for GPU index 1 driver (dict) : the mechanism on how a training is done in a loop ( iterated over n epochs ). Sub-arguments : module (str) : training driver identifier. Supported training driver methods is provided at training driver section args (dict) : the corresponding arguments to the respective training driver module Model \u00b6 Flagged with model key (dict) in the experiment file. This configuration set the selected deep learning model architecture for the specific task. E.g. : model: { name: FPNSSD, preprocess_args: { input_size: 512, input_normalization: { mean: [ 0.5, 0.5, 0.5 ], std: [ 0.5, 0.5, 0.5 ], scaler: 255 } }, network_args: { backbone: shufflenetv2_x1.0, n_classes: 20, pyramid_channels: 256, aspect_ratios: [ 1, 2., 3. ], pretrained_backbone: True, }, loss_args: { neg_pos: 3, overlap_thresh: 0.5, }, postprocess_args: { nms: True, }, init_state_dict: 'experiments/outputs/shufflenetv2x100_fpn_ssd_voc2007_512/16575bd31b364539817177ca14147b5d/shufflenetv2x100_fpn_ssd_voc2007_512-epoch-100.pth } Arguments : name (str) : model name identifier. List of supported models can be found in models zoo section . E.g. : preprocess_args (dict) : configurations for input data preprocessing. Sub-arguments : input_size (int) : input data size, input image will be resized to square while maintaining aspect ratio, by padding the data with black pixel (0,0,0) input_normalization (dict) : determine how input data will be normalized. Input data will be divided by scaler value accross all channel i.e. output = input / scaler . Then, given mean : [M1,...,Mn] and std : [S1,..,Sn] for n channels, this transform will normalize each channel of the input torch.*Tensor i.e. output[channel] = (input[channel] - mean[channel]) / std[channel] mean (list) : sequence of means for each channel. std (list) : sequence of standard deviations for each channel. scaler (list) : scale value for each pixel, default = 255 network_args (dict) : configuration related to the models architecture hyperparameter correspond to the respective model name identifier loss_args (dict) : configuration related to loss calculation hyperparameter which will be used in the training pipeline, which correspond to the respective model name identifier postprocess_args (dict) : configuration related to postprocess arguments, which correspond to the respective model name identifier init_state_dict (str) : path to pretrained model, for transfer learning Graph Exporter \u00b6 Flagged with exporter key (list or dict) in the experiment file. This configuration describe the graph exporter which will be used to convert the Pytorch graph into Intermediate Representation (IR) format. To export graph into multiple IR, use list of dict configurations. E.g. : Single IR exporter : exporter: { module: onnx, args: { opset_version: 11, }, } Multiple IR exporter : exporter: [ { module: onnx, args: { opset_version: 11, }, }, { module: torchscript, args: {}, }, ] Arguments : module (str) : selected exporter module, see exporter section for supported exporter modules args (dict) : the corresponding arguments to the respective exporter module","title":"Experiment File Configuration"},{"location":"user-guides/experiment_file_config/#experiment-file-configuration","text":"A YAML experiment file is needed to navigate all of Vortex pipelines. This experiment file will contain and track all configurations during all pipelines and can be a single source of information on how a model is developed. Several examples of experiment file can be inspected on this link . In this guide, we will cover all available sections that can be configured. All available configurations is listed below :","title":"Experiment File Configuration"},{"location":"user-guides/experiment_file_config/#experiment-name","text":"Flagged with experiment_name key (str) in the experiment file. This field acts as an experiment identifier and is related to the experiment output directory where the trained model,reports, backups,and etc. will be dumped. E.g. : experiment_name: shufflenetv2x100_softmax_cifar10","title":"Experiment Name"},{"location":"user-guides/experiment_file_config/#logging","text":"Flagged with logging key (dict or str( None )) in the experiment file. This field denotes logging providers that can be used for experiment logging, such as tensorboard, comet-ml, etc. The supported logging provider listed in the logging provider section . E.g. : logging: { module: comet_ml, args: { api_key: HGhusyd76hsGiSbt27688, project_name: vortex-classification, workspace: hyperion-rg }, pytz_timezone: Asia/Jakarta } Arguments : module (str) : denotes a specific logging provider module args (dict) : the corresponding arguments for selected module pytz_timezone (str) : the setting of the recorded experiment run timezone in Vortex local system. All timezone settings can be found in this link","title":"Logging"},{"location":"user-guides/experiment_file_config/#output-directory","text":"Flagged with output_directory key (str) in the experiment file. This configuration set the location where experiment's output directory will be created. E.g. : output_directory: experiments/outputs","title":"Output Directory"},{"location":"user-guides/experiment_file_config/#reproducibility-and-cudnn-auto-tune-optional","text":"Flagged with seed key (dict) in the experiment file. This configuration setting can be set if the user wants a model training experiment reproducibility. However as noted in this reference , completely reproducible results are not guaranteed across PyTorch releases, individual commits or different platforms, even with identical seeds. However we still can make the computation deterministic, to produce similar results. For further information, you can check this link . E.g. : seed: { torch: 0, numpy: 0, cudnn: { deterministic: True, benchmark: False, } } Arguments : torch (int) : set the Pytorch seed numpy (int) : set the Numpy seed cudnn (dict) : cuDNN configurations, sub-arguments : deterministic (bool) : set the Pytorch computation to be deterministic benchmark (bool) : set the cuDNN auto-tuner The cudnn args only need to be specified if the user trains using NVIDIA GPU (CuDNN backend). Additionally, by using this seed configuration you also use cuDNN auto-tune capability by using the following configuration seed: { cudnn: { benchmark: True, } }","title":"Reproducibility and cuDNN auto-tune (OPTIONAL)"},{"location":"user-guides/experiment_file_config/#dataset","text":"Flagged with dataset key (dict) in the experiment file. This is the configurations of the dataset, augmentations, and dataloader which will be used in the training. E.g. : dataset: { train: { dataset: VOC0712DetectionDataset, args: { image_set: train, }, augmentations: [ { module: albumentations, args: { transforms: [ { transform: HorizontalFlip, args: { p: 0.5}}, ], bbox_params: { min_visibility: 0.0, min_area: 0.0 }, visual_debug: False } }, # Example if you want to add another augmentation module # { # module: imgaug, # NOTES : `imgaug` module is not implemented yet, just example # args: { # transforms: [] # } # } ] }, eval: { dataset: VOC0712DetectionDataset, args: { image_set: val } }, dataloader: { dataloader: DataLoader, args: { num_workers: 0, batch_size: 16, shuffle: True, }, } } Arguments : train AND eval (str) ( eval is Optional): denotes the configuration of training and validation dataset respectively, if eval is not provided, in-loop validation process will be skipped. eval is mandatory for validation pipeline. Sub-arguments : dataset (str) : the dataset class names which will be used, mentioned in getting started section step 1. args (dict) : the corresponding arguments to the respective dataset class initialization augmentations (list[dict]) ( train only) : the augmentation configurations for training dataset. Augmentation modules provided in the list will be executed sequentially. sub-arguments (list members as dict) : module (str) : selected augmentation module, see augmentation module section for supported augmentation modules args (dict) : the corresponding arguments for selected module dataloader (dict) : denotes the configuration of the dataset iterator dataloader (str) : specify the dataloader module which will be used, supported data loader modules is provided at data loader module section args (dict) : the corresponding arguments for selected dataloader","title":"Dataset"},{"location":"user-guides/experiment_file_config/#trainer","text":"Flagged with trainer key (dict) in the experiment file. This configuration set how we train, validate on training, and several other configurations related to training iterations. E.g. : trainer: { optimizer: { method: SGD, args: { lr: 0.001, momentum: 0.9, weight_decay: 0.0005 } }, scheduler: { method: CosineLRScheduler, args: { t_initial: 200, t_mul: 1.0, lr_min: 0.00001, warmup_lr_init: 0.00001, warmup_t: 3, cycle_limit: 1, t_in_epochs: True, decay_rate: 0.1 } }, validation: { args: { score_threshold: 0.9, iou_threshold: 0.2, }, val_epoch: 10 }, epoch: 200, save_epoch: 1, device: 'cuda:0', driver: { module: DefaultTrainer, args: { accumulation_step: 4, } } } Arguments : optimizer (dict) : configuration for optimization algorithm. Sub-arguments : method (str) : optimization method identifier, currently support all optimizers supported by Pytorch listed in this link args (dict) : the corresponding arguments to the respective optimizer method scheduler (dict) : methods to adjust the learning rate based on the number of epochs method (str) : scheduler method identifier, supported scheduler methods is provided at scheduler section args (dict) : the corresponding arguments to the respective scheduler method validation (dict) (Optional) : additional parameter for validation process inside the training loop, if not provided in-loop validation process will be skipped. Sub-arguments : args (dict) : additional arguments for model postprocessing. Dependent on the described model's task. For detection : score_threshold (float) : threshold applied to the model\u2019s predicted object confidence score. Only objects with prediction scores higher than this threshold will be considered true objects, else considered as background. iou_threshold (float) : threshold for non-maxima suppression (NMS) intersection over union (IoU) For classification : No additional arguments for this task, you can leave args with empty dict {} val_epoch (int) : periodic number of epoch when the validation process will be executed in the training loop epoch (int) : number of dataset iteration (epoch) being done on the training dataset. 1 epoch is 1 dataset iteration save_epoch (int) : number of epoch before a model checkpoint being saved for backup device (str) : set the training device, whether using CPU : cpu or cuda GPU : cuda . you can add :{i} to cuda to point for specific GPU index {i} , E.g. cuda:0 for GPU index 0, cuda:1 for GPU index 1 driver (dict) : the mechanism on how a training is done in a loop ( iterated over n epochs ). Sub-arguments : module (str) : training driver identifier. Supported training driver methods is provided at training driver section args (dict) : the corresponding arguments to the respective training driver module","title":"Trainer"},{"location":"user-guides/experiment_file_config/#model","text":"Flagged with model key (dict) in the experiment file. This configuration set the selected deep learning model architecture for the specific task. E.g. : model: { name: FPNSSD, preprocess_args: { input_size: 512, input_normalization: { mean: [ 0.5, 0.5, 0.5 ], std: [ 0.5, 0.5, 0.5 ], scaler: 255 } }, network_args: { backbone: shufflenetv2_x1.0, n_classes: 20, pyramid_channels: 256, aspect_ratios: [ 1, 2., 3. ], pretrained_backbone: True, }, loss_args: { neg_pos: 3, overlap_thresh: 0.5, }, postprocess_args: { nms: True, }, init_state_dict: 'experiments/outputs/shufflenetv2x100_fpn_ssd_voc2007_512/16575bd31b364539817177ca14147b5d/shufflenetv2x100_fpn_ssd_voc2007_512-epoch-100.pth } Arguments : name (str) : model name identifier. List of supported models can be found in models zoo section . E.g. : preprocess_args (dict) : configurations for input data preprocessing. Sub-arguments : input_size (int) : input data size, input image will be resized to square while maintaining aspect ratio, by padding the data with black pixel (0,0,0) input_normalization (dict) : determine how input data will be normalized. Input data will be divided by scaler value accross all channel i.e. output = input / scaler . Then, given mean : [M1,...,Mn] and std : [S1,..,Sn] for n channels, this transform will normalize each channel of the input torch.*Tensor i.e. output[channel] = (input[channel] - mean[channel]) / std[channel] mean (list) : sequence of means for each channel. std (list) : sequence of standard deviations for each channel. scaler (list) : scale value for each pixel, default = 255 network_args (dict) : configuration related to the models architecture hyperparameter correspond to the respective model name identifier loss_args (dict) : configuration related to loss calculation hyperparameter which will be used in the training pipeline, which correspond to the respective model name identifier postprocess_args (dict) : configuration related to postprocess arguments, which correspond to the respective model name identifier init_state_dict (str) : path to pretrained model, for transfer learning","title":"Model"},{"location":"user-guides/experiment_file_config/#graph-exporter","text":"Flagged with exporter key (list or dict) in the experiment file. This configuration describe the graph exporter which will be used to convert the Pytorch graph into Intermediate Representation (IR) format. To export graph into multiple IR, use list of dict configurations. E.g. : Single IR exporter : exporter: { module: onnx, args: { opset_version: 11, }, } Multiple IR exporter : exporter: [ { module: onnx, args: { opset_version: 11, }, }, { module: torchscript, args: {}, }, ] Arguments : module (str) : selected exporter module, see exporter section for supported exporter modules args (dict) : the corresponding arguments to the respective exporter module","title":"Graph Exporter"},{"location":"user-guides/hypopt_file_config/","text":"Hyperparameter Optimization File Configuration \u00b6 An additional YAML configuration file is needed for hyperparameter optimization configuration. This hypopt configuration file will describe the parameters and objective to be optimized. Several examples of hypopt config file can be inspected on this link In this guide, we will describe the configurations needed for Optuna-based hyperparameter optimization All available configurations is listed below : Study Name \u00b6 Flagged with study_name key (str) in the hypopt config file. This field is used to identify the name of the hypopt attempt. Will be combined with the experiment_name field in the experiment file to identify an Optuna study . E.g. : study_name: detection_param_search Parameters \u00b6 Flagged with parameters key (list[dict]) in the hypopt config file. This field will contain a list of hyperparameters that will be searched. However, each parameter mentioned in here must be declared first in the experiment file using any initial value. To declare a parameter we use a flattened XML structure with dot ( . ) reducer. For example we want to search the parameter of score_threshold and iou_threshold in which their structure in the experiment file is shown below : trainer: { ## optional field for validation step validation: { ## passed to validator class args: { score_threshold: 0.9, iou_threshold: 0.2, }, val_epoch: 5, }, } Thus, we declare these parameters in the hypopt config as shown below: parameters: [ trainer.validation.args.score_threshold: { suggestion: suggest_discrete_uniform, args: { low: 0.05, high: 0.95, q: 0.025, } }, trainer.validation.args.iou_threshold: { suggestion: suggest_discrete_uniform, args: { low: 0.05, high: 0.5, q: 0.05, } }, ] Each parameter presented as (dict) type and have the following key arguments: suggestion (str) : define the Optuna trial object\u2019s suggestion method which will be used, any suggest_* method should be supported. For full reference see this link args (dict) : the corresponding arguments to the respective suggestion function Objective \u00b6 Flagged with objective key (dict) in the hypopt config file. This field denotes the function which we want to optimize, e.g. minimizing training loss or maximizing validation metric. In Vortex, we provide two general objective settings related to development pipelines. E.g. : objective: { module: TrainObjective, args: { metric_type: val, ## [val, loss] metric_name: mean_ap, ## if metric_type==val ## final objective value is the reduced validation metrics reduction: average, # reduction is based on numpy function (e.g. np.average, np.max, etc.) reduction_args: { weights: [ 1, 2, 3, 4, 5 ] } } } Arguments : module (str) : denotes a specific objective function module. Supported objective function is available in the next sub-section args (dict) : the corresponding arguments for selected module Train Objective Optimization \u00b6 This objective aim to optimize hyperparameters on train pipeline . E.g. : objective: { module: TrainObjective, args: { metric_type: val, ## [val, loss] metric_name: mean_ap, ## if metric_type==val ## final objective value is the reduced validation metrics reduction: average, # reduction is based on numpy function (e.g. np.average, np.max, etc.) reduction_args: { weights: [ 1, 2, 3, 4, 5 ] } } } Arguments : metric_type (str) : type of metric to be optimized : val or loss . val metric is extracted from in-training-loop validation process. So the provided experiment file must be valid for validation. loss metric is extracted from training process metric_name (str) : only used if metric_type is set to val . This argument denotes the name of the metric which want to be optimized. The available setting for this argument also related to the model's task Detection task : mean_ap : using the mean-Average Precision metrics Classification task : accuracy : using the accuracy metrics reduction (str) : the reduction function used to averaged the returned value from training pipeline. Supported reduction : latest : select the last value ( index [-1] ), mean : see numpy.mean sum : see numpy.sum min : see numpy.min max : see numpy.max median : see numpy.median average : see numpy.average percentile : see numpy.percentile quantile : see numpy.quantile reduction_args (dict) : the corresponding arguments for selected reduction . Additional Explanation : This objective utilize training pipelines which will return a list of values ( not singular value ), either a sequence of recorded loss value on each epoch, ( 100 epoch means list of 100 loss values) or a sequence of validation metrics ( val_epoch set to 5 and 100 epoch means list of 100/5 = 20 metric values ). However, Optuna expect a singular value to be optimized. Thus, we apply a reduction method using numpy functionality to pick the most representative value for that trial. Important Notes : Be careful when using weighted average, because the weights args expect the user to input a weight array with the same length compared to the input. So you must calculate the length yourself. E.g. : You want to use weighted average reduction on metric_type = loss. epoch is set to 50, so the input length to reduction function is 50 ( each epoch will dump 1 loss value ). Hence you need to provide weights in the reduction_args with the same length You want to use average reduction on metric_type = val. epoch is set to 50 , val_epoch is set to 5 , so the input length to reduction function is 50/5 = 10 ( epoch / val_epoch , each val_epoch will dump 1 validation metrics value). Hence you need to provide weights in the reduction_args with the same length Validation Objective Optimization \u00b6 This objective aim to optimize hyperparameters post-training pipeline (validate, predict) and utilize validation pipeline E.g. : objective: { module: ValidationObjective, args: { metric_name: mean_ap } } Arguments : metric_name (str) : denotes the name of the metric which want to be optimized. The available setting for this argument also related to the model's task Detection task : mean_ap : using the mean-Average Precision metrics Classification task : accuracy : using the accuracy metrics Study \u00b6 Flagged with objective key (dict) in the hypopt config file. This field set the Optuna study initialization. As stated in this link , a study corresponds to an optimization task, i.e. a set of trials. E.g. : study: { n_trials: 5, direction: maximize, pruner: { method: MedianPruner, args: {}, }, sampler: { method: TPESampler, args: {}, }, args: { storage: sqlite://anysqldb_url.db, load_if_exists: True, } } Arguments : n_trials (int) : number of trials attempted in a study direction (str) : either maximize or minimize the objective value pruner (dict) (Optional): enable Optuna pruner configuration, to judge whether the trial should be pruned based on the reported values, full reference in this link . Sub-arguments : method (str) : specify the Pruner\u2019s method which is going to be used args (dict) : the corresponding arguments to the respective pruner method sampler (dict) (Optional) : enable Optuna different sampler to sample the combination of hyperparameters value in a search space, full reference in this link . Sub-arguments : method (str) : specify the Sampler\u2019s method which is going to be used args (dict) : the corresponding arguments to the respective sampler method args (dict) (Optional) : Additional Optuna Study\u2019s arguments to use database to save studies. Sub-arguments : storage (str) : database URL. If this argument is set to None, in-memory storage is used, and the Study will not be persistent, full reference in this link load_if_exists (str) : flag to control the behavior to handle a conflict of study names, full reference in this link Additional Parameters' Override \u00b6 This config is used to override any experiment file configuration besides the search parameters mentioned in parameters section . For example, using epoch = 200 in initial experiment file config is too long if we want to set the n_trials = 20 ( means 20 * 200 epoch without any sampler ), so for hypopt we may only use 10 epochs. In order to do that we assign the overridden parameter in this field with flattened XML structure similar to parameters section E.g.: override: { trainer.epoch: 10, trainer.validation.val_epoch: 2, } Additional Experiment Configuration \u00b6 Additional experiment configuration which wants to be added to the original experiment file while optimized. Mandatory but can be empty E.g. : additional_config: {}","title":"HypOpt File Configuration"},{"location":"user-guides/hypopt_file_config/#hyperparameter-optimization-file-configuration","text":"An additional YAML configuration file is needed for hyperparameter optimization configuration. This hypopt configuration file will describe the parameters and objective to be optimized. Several examples of hypopt config file can be inspected on this link In this guide, we will describe the configurations needed for Optuna-based hyperparameter optimization All available configurations is listed below :","title":"Hyperparameter Optimization File Configuration"},{"location":"user-guides/hypopt_file_config/#study-name","text":"Flagged with study_name key (str) in the hypopt config file. This field is used to identify the name of the hypopt attempt. Will be combined with the experiment_name field in the experiment file to identify an Optuna study . E.g. : study_name: detection_param_search","title":"Study Name"},{"location":"user-guides/hypopt_file_config/#parameters","text":"Flagged with parameters key (list[dict]) in the hypopt config file. This field will contain a list of hyperparameters that will be searched. However, each parameter mentioned in here must be declared first in the experiment file using any initial value. To declare a parameter we use a flattened XML structure with dot ( . ) reducer. For example we want to search the parameter of score_threshold and iou_threshold in which their structure in the experiment file is shown below : trainer: { ## optional field for validation step validation: { ## passed to validator class args: { score_threshold: 0.9, iou_threshold: 0.2, }, val_epoch: 5, }, } Thus, we declare these parameters in the hypopt config as shown below: parameters: [ trainer.validation.args.score_threshold: { suggestion: suggest_discrete_uniform, args: { low: 0.05, high: 0.95, q: 0.025, } }, trainer.validation.args.iou_threshold: { suggestion: suggest_discrete_uniform, args: { low: 0.05, high: 0.5, q: 0.05, } }, ] Each parameter presented as (dict) type and have the following key arguments: suggestion (str) : define the Optuna trial object\u2019s suggestion method which will be used, any suggest_* method should be supported. For full reference see this link args (dict) : the corresponding arguments to the respective suggestion function","title":"Parameters"},{"location":"user-guides/hypopt_file_config/#objective","text":"Flagged with objective key (dict) in the hypopt config file. This field denotes the function which we want to optimize, e.g. minimizing training loss or maximizing validation metric. In Vortex, we provide two general objective settings related to development pipelines. E.g. : objective: { module: TrainObjective, args: { metric_type: val, ## [val, loss] metric_name: mean_ap, ## if metric_type==val ## final objective value is the reduced validation metrics reduction: average, # reduction is based on numpy function (e.g. np.average, np.max, etc.) reduction_args: { weights: [ 1, 2, 3, 4, 5 ] } } } Arguments : module (str) : denotes a specific objective function module. Supported objective function is available in the next sub-section args (dict) : the corresponding arguments for selected module","title":"Objective"},{"location":"user-guides/hypopt_file_config/#train-objective-optimization","text":"This objective aim to optimize hyperparameters on train pipeline . E.g. : objective: { module: TrainObjective, args: { metric_type: val, ## [val, loss] metric_name: mean_ap, ## if metric_type==val ## final objective value is the reduced validation metrics reduction: average, # reduction is based on numpy function (e.g. np.average, np.max, etc.) reduction_args: { weights: [ 1, 2, 3, 4, 5 ] } } } Arguments : metric_type (str) : type of metric to be optimized : val or loss . val metric is extracted from in-training-loop validation process. So the provided experiment file must be valid for validation. loss metric is extracted from training process metric_name (str) : only used if metric_type is set to val . This argument denotes the name of the metric which want to be optimized. The available setting for this argument also related to the model's task Detection task : mean_ap : using the mean-Average Precision metrics Classification task : accuracy : using the accuracy metrics reduction (str) : the reduction function used to averaged the returned value from training pipeline. Supported reduction : latest : select the last value ( index [-1] ), mean : see numpy.mean sum : see numpy.sum min : see numpy.min max : see numpy.max median : see numpy.median average : see numpy.average percentile : see numpy.percentile quantile : see numpy.quantile reduction_args (dict) : the corresponding arguments for selected reduction . Additional Explanation : This objective utilize training pipelines which will return a list of values ( not singular value ), either a sequence of recorded loss value on each epoch, ( 100 epoch means list of 100 loss values) or a sequence of validation metrics ( val_epoch set to 5 and 100 epoch means list of 100/5 = 20 metric values ). However, Optuna expect a singular value to be optimized. Thus, we apply a reduction method using numpy functionality to pick the most representative value for that trial. Important Notes : Be careful when using weighted average, because the weights args expect the user to input a weight array with the same length compared to the input. So you must calculate the length yourself. E.g. : You want to use weighted average reduction on metric_type = loss. epoch is set to 50, so the input length to reduction function is 50 ( each epoch will dump 1 loss value ). Hence you need to provide weights in the reduction_args with the same length You want to use average reduction on metric_type = val. epoch is set to 50 , val_epoch is set to 5 , so the input length to reduction function is 50/5 = 10 ( epoch / val_epoch , each val_epoch will dump 1 validation metrics value). Hence you need to provide weights in the reduction_args with the same length","title":"Train Objective Optimization"},{"location":"user-guides/hypopt_file_config/#validation-objective-optimization","text":"This objective aim to optimize hyperparameters post-training pipeline (validate, predict) and utilize validation pipeline E.g. : objective: { module: ValidationObjective, args: { metric_name: mean_ap } } Arguments : metric_name (str) : denotes the name of the metric which want to be optimized. The available setting for this argument also related to the model's task Detection task : mean_ap : using the mean-Average Precision metrics Classification task : accuracy : using the accuracy metrics","title":"Validation Objective Optimization"},{"location":"user-guides/hypopt_file_config/#study","text":"Flagged with objective key (dict) in the hypopt config file. This field set the Optuna study initialization. As stated in this link , a study corresponds to an optimization task, i.e. a set of trials. E.g. : study: { n_trials: 5, direction: maximize, pruner: { method: MedianPruner, args: {}, }, sampler: { method: TPESampler, args: {}, }, args: { storage: sqlite://anysqldb_url.db, load_if_exists: True, } } Arguments : n_trials (int) : number of trials attempted in a study direction (str) : either maximize or minimize the objective value pruner (dict) (Optional): enable Optuna pruner configuration, to judge whether the trial should be pruned based on the reported values, full reference in this link . Sub-arguments : method (str) : specify the Pruner\u2019s method which is going to be used args (dict) : the corresponding arguments to the respective pruner method sampler (dict) (Optional) : enable Optuna different sampler to sample the combination of hyperparameters value in a search space, full reference in this link . Sub-arguments : method (str) : specify the Sampler\u2019s method which is going to be used args (dict) : the corresponding arguments to the respective sampler method args (dict) (Optional) : Additional Optuna Study\u2019s arguments to use database to save studies. Sub-arguments : storage (str) : database URL. If this argument is set to None, in-memory storage is used, and the Study will not be persistent, full reference in this link load_if_exists (str) : flag to control the behavior to handle a conflict of study names, full reference in this link","title":"Study"},{"location":"user-guides/hypopt_file_config/#additional-parameters-override","text":"This config is used to override any experiment file configuration besides the search parameters mentioned in parameters section . For example, using epoch = 200 in initial experiment file config is too long if we want to set the n_trials = 20 ( means 20 * 200 epoch without any sampler ), so for hypopt we may only use 10 epochs. In order to do that we assign the overridden parameter in this field with flattened XML structure similar to parameters section E.g.: override: { trainer.epoch: 10, trainer.validation.val_epoch: 2, }","title":"Additional Parameters' Override"},{"location":"user-guides/hypopt_file_config/#additional-experiment-configuration","text":"Additional experiment configuration which wants to be added to the original experiment file while optimized. Mandatory but can be empty E.g. : additional_config: {}","title":"Additional Experiment Configuration"},{"location":"user-guides/pipelines/","text":"Vortex Pipelines \u00b6 This section will describe how to easily run each of Vortex pipeline in details. For complete pipelines flow please see Vortex overview section Training Pipeline \u00b6 This pipeline purpose is to train a deep learning model using the provided dataset. If you need to integrate the training into your own script you can see the training pipeline API section . To run this pipeline, make sure you've already prepared : Dataset : see this section for built-in datasets, or this section for external datasets Experiment file : see this section to create one You only need to run this command from the command line interface : usage: vortex train [-h] -c CONFIG [--no-log] Vortex training pipeline; will generate a Pytorch model file optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG path to experiment config file --no-log disable logging, ignore experiment file config E.g. : vortex train -c experiments/config/efficientnet_b0_classification_cifar10.yml This pipeline will generate several outputs : Local runs log file : every time a user runs a VORTEX training experiment, the experiment logger module will write a local file experiments/local_runs.log which will record all experimental training runs which have already executed sequentially for easier tracking. Example of the content inside experiments/local_runs.log is shown below : ############################################################################### Timestamp : 03/27/2020, 09:24:00 Experiment Name : test_torchvision_dataset Output Path : experiments/outputs/test_torchvision_dataset/601f45782a884286be310b1ffe562597 Logging Provider : comet_ml Experiment Log URL : https://www.comet.ml/hyperion-rg/vortex-dev/601f45782a884286be310b1ffe562597 ############################################################################### Experiment directory : If not exist yet, training script will make a directory under the configured output_directory path. The created directory will be named after the experiment_name configuration and will be the directory to dump training (final weight), validation result (if any), backup, etc. Run directory : Everytime user runs the training script, it will be tagged as a new experiment run. Vortex (or third party logger) will generate a unique key which will be an identifier for that specific experiment run. And thus, Vortex will make a new directory under the experiment directory which will act as a backup directory. It will store the duplicate of the executed experiment file (as a backup) and will be the directory which store intermediate model\u2019s weight path (weight that saved every n-epoch). For example, in the previous example log the output path is : Output Path : experiments/outputs/test_torchvision_dataset/601f45782a884286be310b1ffe562597 The experiment directory is test_torchvision_dataset and the run directory is 601f45782a884286be310b1ffe562597 Backup experiment file : Experiment file will be duplicated and stored under run directory Intermediate model weight : Model\u2019s weight will be dumped into run directory every n -epoch which is configured in save_epoch in experiment file with .pth extension Final model weight : Model\u2019s weight after all training epoch is completed will be dumped in the experiment directory with .pth extension Experiment log : If logging is enabled, training metrics will be collected by the logging provider. Additionally if the config file is valid for validation, the validation metrics will also be collected. Validation Pipeline \u00b6 This pipeline handle the evaluation of the Vortex model (Pytorch state dict .pth ) in term of model's performance and resource usage. In addition, this pipeline also generate a visual report. If you need to integrate the validation into your own script you can see the validation pipeline API section . To run this pipeline, make sure you've already prepared : Validation dataset : see this section for built-in datasets, or this section for external datasets Experiment file : see this section to create one. Must be valid for validation, make sure dataset.eval and trainer.validation is set Vortex model's file *.pth : obtained from training pipeline which corresponds to the previous mentioned experiment file You only need to run this command from the command line interface : usage: vortex validate [-h] -c CONFIG [-w WEIGHTS] [-v] [--quiet] [-d [DEVICES [DEVICES ...]]] [-b BATCH_SIZE] Vortex Pytorch model validation pipeline; successful runs will produce autogenerated reports optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG path to experiment config -w WEIGHTS, --weights WEIGHTS path to selected weights(optional, will be inferred from `output_directory` and `experiment_name` field from config) if not specified -v, --verbose verbose prediction output --quiet -d [DEVICES [DEVICES ...]], --devices [DEVICES [DEVICES ...]] computation device to be used for prediction, possible to list multiple devices -b BATCH_SIZE, --batch-size BATCH_SIZE batch size for validation NOTES : if --weights is not provided, Vortex will assume final weights exist in the experiment directory E.g. : vortex validate -c experiments/config/efficientnet_b0_classification_cifar10.yml \\ -b 8 \\ -d cpu cuda This pipeline will generate several outputs : Report file : after successful evaluation, report file will be generated under directory reports in the experiment directory based on experiment_name under output_directory . Pro Tip : the generated report could be easily converted to pdf using pandoc or vscode markdown-pdf extension . Prediction Pipeline \u00b6 This pipeline is used to test and visualize your Vortex model's prediction. If you need to integrate the prediction into your own script you can see the prediction pipeline API section . To run this pipeline, make sure you've already prepared : Experiment file : see this section to create one Vortex model's file *.pth : obtained from training pipeline which corresponds to the previous mentioned experiment file Input image(s) : image file(s) (tested with *.jpg , *.jpeg , *.png extension) You only need to run this command from the command line interface : usage: vortex predict [-h] -c CONFIG [-w WEIGHTS] [-o OUTPUT_DIR] -i IMAGE [IMAGE ...] [-d DEVICE] [--score_threshold SCORE_THRESHOLD] [--iou_threshold IOU_THRESHOLD] Vortex Pytorch model prediction pipeline; may receive multiple image(s) for batched prediction optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG path to experiment config -w WEIGHTS, --weights WEIGHTS path to selected weights(optional, will be inferred from `output_directory` and `experiment_name` field from config) if not specified -o OUTPUT_DIR, --output-dir OUTPUT_DIR directory to dump prediction visualization -i IMAGE [IMAGE ...], --image IMAGE [IMAGE ...] path to test image(s) -d DEVICE, --device DEVICE the device in which the inference will be performed --score_threshold SCORE_THRESHOLD score threshold for detection, only used if model is detection, ignored otherwise --iou_threshold IOU_THRESHOLD iou threshold for nms, , only used if model is detection, ignored otherwise NOTES : if --weights is not provided, Vortex will assume final weights exist in the experiment directory E.g. : vortex predict -c experiments/config/efficientnet_b0_classification_cifar10.yml \\ -i image1.jpg image2.jpg \\ -d cuda \\ -o output_vis NOTES : Provided multiple input images will be treated as batch input This pipeline will generate several outputs : Output Visualization Directory : if --output_dir is provided, it will create the directory in your current working directory Prediction Visualization : prediction visualization will be generated in the --output_dir if provided, or in the current working dir if not. The generated file will have prediction_ name prefix. Hyperparameters Optimization Pipeline \u00b6 This pipeline is used to search for optimum hyperparameter to be used for either training pipeline or validation pipeline (parameter in validation pipeline also can be used for prediction pipeline). Basically this pipeline is Optuna wrapper for Vortex components. If you need to integrate the prediction into your own script you can see the hyperparameters optimization pipeline API section . To run this pipeline, make sure you've already prepared : Hypopt config file : see this section to create one Experiment file : see this section to create one Vortex model's file *.pth : obtained from training pipeline which corresponds to the previous mentioned experiment file Related objective's pipeline requirement You only need to run this command from the command line interface : usage: vortex hypopt [-h] -c CONFIG -o OPTCONFIG [-w WEIGHTS] Vortex hyperparameter optimization experiment optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG path to experiment config file -o OPTCONFIG, --optconfig OPTCONFIG path to hypopt config file -w WEIGHTS, --weights WEIGHTS path to selected weights (optional, will be inferred from `output_directory` and `experiment_name` field from config) if not specified, valid only for ValidationObjective, ignored otherwise NOTES : if --weights is not provided, Vortex will assume final weights exist in the experiment directory E.g. : vortex hypopt -c experiments/config/efficientnet_b0_classification_cifar10.yml \\ -o experiments/hypopt/learning_rate_search.yml This pipeline will generate several outputs : Hypopt Output Dir : hypopt/{hypopt_study_name} will be created under experiment directory Best Parameters : file *.txt containing best parameters will be created in hypopt output dir Hypopt Visualization : graph visualization of parameter search (visualization extension must be installed. see installation section ) will be created in hypopt output dir Graph Export Pipeline \u00b6 This pipeline is used to export trained Vortex model (or graph) into another graph representation (or Intermediate Representation (IR)). If you need to integrate the graph export pipeline into your own script you can see the graph export pipeline API section . To run this pipeline, make sure you've already prepared : Experiment file : see this section to create one and make sure the exporter section is already configured Vortex model's file *.pth : obtained from training pipeline which corresponds to the previous mentioned experiment file Example Input Image : example input image for correct graph tracing. Recommended for using image from training dataset and strongly recommended for model with detection task You only need to run this command from the command line interface : usage: vortex export [-h] -c CONFIG [-w WEIGHTS] [-i EXAMPLE_INPUT] export model to specific IR specified in config, output IR are stored in the experiment directory based on `experiment_name` under `output_directory` config field, after successful export, you should be able to visualize the network using [netron](https://lutzroeder.github.io/netron/) optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG export experiment config file -w WEIGHTS, --weights WEIGHTS path to selected weights (optional, will be inferred from `output_directory` and `experiment_name` field from config) if not specified -i EXAMPLE_INPUT, --example-input EXAMPLE_INPUT path to example input for tracing (optional, may be necessary for correct tracing, especially for detection model) NOTES : if --weights is not provided, Vortex will assume final weights exist in the experiment directory E.g. : vortex export -c experiments/config/efficientnet_b0_classification_cifar10.yml -i image1.jpg This pipeline will generate several outputs : IR model file : IR model file will be created under experiment directory , with file extension that correspond to exporter settings IR Validation Pipeline \u00b6 This pipeline handle the evaluation of the IR model ( *.pt or *.onnx ) in term of model's performance and resource usage. In addition, this pipeline also generate a visual report. If you need to integrate the validation into your own script you can see the IR validation pipeline API section . To run this pipeline, make sure you've already prepared : Validation dataset : see this section for built-in datasets, or this section for external datasets Experiment file : see this section to create one. Must be valid for validation, make sure dataset.eval and trainer.validation is set IR model file *.pt or *.onnx : obtained from graph export pipeline IR runtime library and environment : make sure runtime library and environment is installed (currently runtime library installed together with vortex) You only need to run this command from the command line interface : usage: vortex ir_runtime_validate [-h] -c CONFIG -m MODEL [-r [RUNTIME [RUNTIME ...]]] [-v] [--quiet] [--batch-size BATCH_SIZE] Vortex exported IR graph validation pipeline; successful runs will produce autogenerated reports optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG path to experiment config including dataset fields, must be valid for validation, dataset.eval will be used for evaluation -m MODEL, --model MODEL path to IR model -r [RUNTIME [RUNTIME ...]], --runtime [RUNTIME [RUNTIME ...]] runtime backend device -v, --verbose verbose prediction output --quiet --batch-size BATCH_SIZE batch size for validation; NOTE : passed value should be matched with exported model batch size E.g. : vortex ir_runtime_validate -c experiments/config/efficientnet_b0_classification_cifar10.yml \\ -m experiments/outputs/efficientnet_b0_classification_cifar10/efficientnet_b0_classification_cifar10.pt \\ -b 8 \\ -r cpu cuda This pipeline will generate several outputs : Report file : after successful evaluation, report file will be generated under directory reports in the experiment directory based on experiment_name under output_directory . Pro Tip : the generated report could be easily converted to pdf using pandoc or vscode markdown-pdf extension . IR Prediction Pipeline \u00b6 This pipeline is used to test and visualize your IR model's ( *.pt or *.onnx ) prediction. If you need to integrate the prediction into your own script you can see the IR prediction pipeline API section . To run this pipeline, make sure you've already prepared : IR model file *.pt or *.onnx : obtained from graph export pipeline IR runtime library and environment : make sure runtime library and environment is installed (currently runtime library installed together with vortex) Input image(s) : image file(s) (tested with *.jpg , *.jpeg , *.png extension) You only need to run this command from the command line interface : usage: vortex ir_runtime_predict [-h] -m MODEL -i IMAGE [IMAGE ...] [-o OUTPUT_DIR] [--score_threshold SCORE_THRESHOLD] [--iou_threshold IOU_THRESHOLD] [-r RUNTIME] Vortex IR model prediction pipeline; may receive multiple image(s) for batched prediction optional arguments: -h, --help show this help message and exit -m MODEL, --model MODEL path to IR model -i IMAGE [IMAGE ...], --image IMAGE [IMAGE ...] path to test image(s); at least 1 path should be provided, supports up to model batch_size -o OUTPUT_DIR, --output-dir OUTPUT_DIR directory to dump prediction visualization --score_threshold SCORE_THRESHOLD score threshold for detection, only used if model is detection, ignored otherwise --iou_threshold IOU_THRESHOLD iou threshold for nms, only used if model is detection, ignored otherwise -r RUNTIME, --runtime RUNTIME runtime device E.g. : vortex ir_runtime_predict -c experiments/config/efficientnet_b0_classification_cifar10.yml \\ -m experiments/outputs/efficientnet_b0_classification_cifar10/efficientnet_b0_classification_cifar10.pt \\ -i image1.jpg image2.jpg \\ -r cuda \\ -o output_vis NOTES : Provided multiple input images will be treated as batch input. Vortex IR model is strict with batch size, means that provided input batch size must match with Vortex IR exporter batch size configuration. This pipeline will generate several outputs : Output Visualization Directory : if --output_dir is provided, it will create the directory in your current working directory Prediction Visualization : prediction visualization will be generated in the --output_dir if provided, or in the current working dir if not. The generated file will have prediction_ name prefix.","title":"Pipelines"},{"location":"user-guides/pipelines/#vortex-pipelines","text":"This section will describe how to easily run each of Vortex pipeline in details. For complete pipelines flow please see Vortex overview section","title":"Vortex Pipelines"},{"location":"user-guides/pipelines/#training-pipeline","text":"This pipeline purpose is to train a deep learning model using the provided dataset. If you need to integrate the training into your own script you can see the training pipeline API section . To run this pipeline, make sure you've already prepared : Dataset : see this section for built-in datasets, or this section for external datasets Experiment file : see this section to create one You only need to run this command from the command line interface : usage: vortex train [-h] -c CONFIG [--no-log] Vortex training pipeline; will generate a Pytorch model file optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG path to experiment config file --no-log disable logging, ignore experiment file config E.g. : vortex train -c experiments/config/efficientnet_b0_classification_cifar10.yml This pipeline will generate several outputs : Local runs log file : every time a user runs a VORTEX training experiment, the experiment logger module will write a local file experiments/local_runs.log which will record all experimental training runs which have already executed sequentially for easier tracking. Example of the content inside experiments/local_runs.log is shown below : ############################################################################### Timestamp : 03/27/2020, 09:24:00 Experiment Name : test_torchvision_dataset Output Path : experiments/outputs/test_torchvision_dataset/601f45782a884286be310b1ffe562597 Logging Provider : comet_ml Experiment Log URL : https://www.comet.ml/hyperion-rg/vortex-dev/601f45782a884286be310b1ffe562597 ############################################################################### Experiment directory : If not exist yet, training script will make a directory under the configured output_directory path. The created directory will be named after the experiment_name configuration and will be the directory to dump training (final weight), validation result (if any), backup, etc. Run directory : Everytime user runs the training script, it will be tagged as a new experiment run. Vortex (or third party logger) will generate a unique key which will be an identifier for that specific experiment run. And thus, Vortex will make a new directory under the experiment directory which will act as a backup directory. It will store the duplicate of the executed experiment file (as a backup) and will be the directory which store intermediate model\u2019s weight path (weight that saved every n-epoch). For example, in the previous example log the output path is : Output Path : experiments/outputs/test_torchvision_dataset/601f45782a884286be310b1ffe562597 The experiment directory is test_torchvision_dataset and the run directory is 601f45782a884286be310b1ffe562597 Backup experiment file : Experiment file will be duplicated and stored under run directory Intermediate model weight : Model\u2019s weight will be dumped into run directory every n -epoch which is configured in save_epoch in experiment file with .pth extension Final model weight : Model\u2019s weight after all training epoch is completed will be dumped in the experiment directory with .pth extension Experiment log : If logging is enabled, training metrics will be collected by the logging provider. Additionally if the config file is valid for validation, the validation metrics will also be collected.","title":"Training Pipeline"},{"location":"user-guides/pipelines/#validation-pipeline","text":"This pipeline handle the evaluation of the Vortex model (Pytorch state dict .pth ) in term of model's performance and resource usage. In addition, this pipeline also generate a visual report. If you need to integrate the validation into your own script you can see the validation pipeline API section . To run this pipeline, make sure you've already prepared : Validation dataset : see this section for built-in datasets, or this section for external datasets Experiment file : see this section to create one. Must be valid for validation, make sure dataset.eval and trainer.validation is set Vortex model's file *.pth : obtained from training pipeline which corresponds to the previous mentioned experiment file You only need to run this command from the command line interface : usage: vortex validate [-h] -c CONFIG [-w WEIGHTS] [-v] [--quiet] [-d [DEVICES [DEVICES ...]]] [-b BATCH_SIZE] Vortex Pytorch model validation pipeline; successful runs will produce autogenerated reports optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG path to experiment config -w WEIGHTS, --weights WEIGHTS path to selected weights(optional, will be inferred from `output_directory` and `experiment_name` field from config) if not specified -v, --verbose verbose prediction output --quiet -d [DEVICES [DEVICES ...]], --devices [DEVICES [DEVICES ...]] computation device to be used for prediction, possible to list multiple devices -b BATCH_SIZE, --batch-size BATCH_SIZE batch size for validation NOTES : if --weights is not provided, Vortex will assume final weights exist in the experiment directory E.g. : vortex validate -c experiments/config/efficientnet_b0_classification_cifar10.yml \\ -b 8 \\ -d cpu cuda This pipeline will generate several outputs : Report file : after successful evaluation, report file will be generated under directory reports in the experiment directory based on experiment_name under output_directory . Pro Tip : the generated report could be easily converted to pdf using pandoc or vscode markdown-pdf extension .","title":"Validation Pipeline"},{"location":"user-guides/pipelines/#prediction-pipeline","text":"This pipeline is used to test and visualize your Vortex model's prediction. If you need to integrate the prediction into your own script you can see the prediction pipeline API section . To run this pipeline, make sure you've already prepared : Experiment file : see this section to create one Vortex model's file *.pth : obtained from training pipeline which corresponds to the previous mentioned experiment file Input image(s) : image file(s) (tested with *.jpg , *.jpeg , *.png extension) You only need to run this command from the command line interface : usage: vortex predict [-h] -c CONFIG [-w WEIGHTS] [-o OUTPUT_DIR] -i IMAGE [IMAGE ...] [-d DEVICE] [--score_threshold SCORE_THRESHOLD] [--iou_threshold IOU_THRESHOLD] Vortex Pytorch model prediction pipeline; may receive multiple image(s) for batched prediction optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG path to experiment config -w WEIGHTS, --weights WEIGHTS path to selected weights(optional, will be inferred from `output_directory` and `experiment_name` field from config) if not specified -o OUTPUT_DIR, --output-dir OUTPUT_DIR directory to dump prediction visualization -i IMAGE [IMAGE ...], --image IMAGE [IMAGE ...] path to test image(s) -d DEVICE, --device DEVICE the device in which the inference will be performed --score_threshold SCORE_THRESHOLD score threshold for detection, only used if model is detection, ignored otherwise --iou_threshold IOU_THRESHOLD iou threshold for nms, , only used if model is detection, ignored otherwise NOTES : if --weights is not provided, Vortex will assume final weights exist in the experiment directory E.g. : vortex predict -c experiments/config/efficientnet_b0_classification_cifar10.yml \\ -i image1.jpg image2.jpg \\ -d cuda \\ -o output_vis NOTES : Provided multiple input images will be treated as batch input This pipeline will generate several outputs : Output Visualization Directory : if --output_dir is provided, it will create the directory in your current working directory Prediction Visualization : prediction visualization will be generated in the --output_dir if provided, or in the current working dir if not. The generated file will have prediction_ name prefix.","title":"Prediction Pipeline"},{"location":"user-guides/pipelines/#hyperparameters-optimization-pipeline","text":"This pipeline is used to search for optimum hyperparameter to be used for either training pipeline or validation pipeline (parameter in validation pipeline also can be used for prediction pipeline). Basically this pipeline is Optuna wrapper for Vortex components. If you need to integrate the prediction into your own script you can see the hyperparameters optimization pipeline API section . To run this pipeline, make sure you've already prepared : Hypopt config file : see this section to create one Experiment file : see this section to create one Vortex model's file *.pth : obtained from training pipeline which corresponds to the previous mentioned experiment file Related objective's pipeline requirement You only need to run this command from the command line interface : usage: vortex hypopt [-h] -c CONFIG -o OPTCONFIG [-w WEIGHTS] Vortex hyperparameter optimization experiment optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG path to experiment config file -o OPTCONFIG, --optconfig OPTCONFIG path to hypopt config file -w WEIGHTS, --weights WEIGHTS path to selected weights (optional, will be inferred from `output_directory` and `experiment_name` field from config) if not specified, valid only for ValidationObjective, ignored otherwise NOTES : if --weights is not provided, Vortex will assume final weights exist in the experiment directory E.g. : vortex hypopt -c experiments/config/efficientnet_b0_classification_cifar10.yml \\ -o experiments/hypopt/learning_rate_search.yml This pipeline will generate several outputs : Hypopt Output Dir : hypopt/{hypopt_study_name} will be created under experiment directory Best Parameters : file *.txt containing best parameters will be created in hypopt output dir Hypopt Visualization : graph visualization of parameter search (visualization extension must be installed. see installation section ) will be created in hypopt output dir","title":"Hyperparameters Optimization Pipeline"},{"location":"user-guides/pipelines/#graph-export-pipeline","text":"This pipeline is used to export trained Vortex model (or graph) into another graph representation (or Intermediate Representation (IR)). If you need to integrate the graph export pipeline into your own script you can see the graph export pipeline API section . To run this pipeline, make sure you've already prepared : Experiment file : see this section to create one and make sure the exporter section is already configured Vortex model's file *.pth : obtained from training pipeline which corresponds to the previous mentioned experiment file Example Input Image : example input image for correct graph tracing. Recommended for using image from training dataset and strongly recommended for model with detection task You only need to run this command from the command line interface : usage: vortex export [-h] -c CONFIG [-w WEIGHTS] [-i EXAMPLE_INPUT] export model to specific IR specified in config, output IR are stored in the experiment directory based on `experiment_name` under `output_directory` config field, after successful export, you should be able to visualize the network using [netron](https://lutzroeder.github.io/netron/) optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG export experiment config file -w WEIGHTS, --weights WEIGHTS path to selected weights (optional, will be inferred from `output_directory` and `experiment_name` field from config) if not specified -i EXAMPLE_INPUT, --example-input EXAMPLE_INPUT path to example input for tracing (optional, may be necessary for correct tracing, especially for detection model) NOTES : if --weights is not provided, Vortex will assume final weights exist in the experiment directory E.g. : vortex export -c experiments/config/efficientnet_b0_classification_cifar10.yml -i image1.jpg This pipeline will generate several outputs : IR model file : IR model file will be created under experiment directory , with file extension that correspond to exporter settings","title":"Graph Export Pipeline"},{"location":"user-guides/pipelines/#ir-validation-pipeline","text":"This pipeline handle the evaluation of the IR model ( *.pt or *.onnx ) in term of model's performance and resource usage. In addition, this pipeline also generate a visual report. If you need to integrate the validation into your own script you can see the IR validation pipeline API section . To run this pipeline, make sure you've already prepared : Validation dataset : see this section for built-in datasets, or this section for external datasets Experiment file : see this section to create one. Must be valid for validation, make sure dataset.eval and trainer.validation is set IR model file *.pt or *.onnx : obtained from graph export pipeline IR runtime library and environment : make sure runtime library and environment is installed (currently runtime library installed together with vortex) You only need to run this command from the command line interface : usage: vortex ir_runtime_validate [-h] -c CONFIG -m MODEL [-r [RUNTIME [RUNTIME ...]]] [-v] [--quiet] [--batch-size BATCH_SIZE] Vortex exported IR graph validation pipeline; successful runs will produce autogenerated reports optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG path to experiment config including dataset fields, must be valid for validation, dataset.eval will be used for evaluation -m MODEL, --model MODEL path to IR model -r [RUNTIME [RUNTIME ...]], --runtime [RUNTIME [RUNTIME ...]] runtime backend device -v, --verbose verbose prediction output --quiet --batch-size BATCH_SIZE batch size for validation; NOTE : passed value should be matched with exported model batch size E.g. : vortex ir_runtime_validate -c experiments/config/efficientnet_b0_classification_cifar10.yml \\ -m experiments/outputs/efficientnet_b0_classification_cifar10/efficientnet_b0_classification_cifar10.pt \\ -b 8 \\ -r cpu cuda This pipeline will generate several outputs : Report file : after successful evaluation, report file will be generated under directory reports in the experiment directory based on experiment_name under output_directory . Pro Tip : the generated report could be easily converted to pdf using pandoc or vscode markdown-pdf extension .","title":"IR Validation Pipeline"},{"location":"user-guides/pipelines/#ir-prediction-pipeline","text":"This pipeline is used to test and visualize your IR model's ( *.pt or *.onnx ) prediction. If you need to integrate the prediction into your own script you can see the IR prediction pipeline API section . To run this pipeline, make sure you've already prepared : IR model file *.pt or *.onnx : obtained from graph export pipeline IR runtime library and environment : make sure runtime library and environment is installed (currently runtime library installed together with vortex) Input image(s) : image file(s) (tested with *.jpg , *.jpeg , *.png extension) You only need to run this command from the command line interface : usage: vortex ir_runtime_predict [-h] -m MODEL -i IMAGE [IMAGE ...] [-o OUTPUT_DIR] [--score_threshold SCORE_THRESHOLD] [--iou_threshold IOU_THRESHOLD] [-r RUNTIME] Vortex IR model prediction pipeline; may receive multiple image(s) for batched prediction optional arguments: -h, --help show this help message and exit -m MODEL, --model MODEL path to IR model -i IMAGE [IMAGE ...], --image IMAGE [IMAGE ...] path to test image(s); at least 1 path should be provided, supports up to model batch_size -o OUTPUT_DIR, --output-dir OUTPUT_DIR directory to dump prediction visualization --score_threshold SCORE_THRESHOLD score threshold for detection, only used if model is detection, ignored otherwise --iou_threshold IOU_THRESHOLD iou threshold for nms, only used if model is detection, ignored otherwise -r RUNTIME, --runtime RUNTIME runtime device E.g. : vortex ir_runtime_predict -c experiments/config/efficientnet_b0_classification_cifar10.yml \\ -m experiments/outputs/efficientnet_b0_classification_cifar10/efficientnet_b0_classification_cifar10.pt \\ -i image1.jpg image2.jpg \\ -r cuda \\ -o output_vis NOTES : Provided multiple input images will be treated as batch input. Vortex IR model is strict with batch size, means that provided input batch size must match with Vortex IR exporter batch size configuration. This pipeline will generate several outputs : Output Visualization Directory : if --output_dir is provided, it will create the directory in your current working directory Prediction Visualization : prediction visualization will be generated in the --output_dir if provided, or in the current working dir if not. The generated file will have prediction_ name prefix.","title":"IR Prediction Pipeline"}]}